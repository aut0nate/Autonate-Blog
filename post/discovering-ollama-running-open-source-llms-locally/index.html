<!doctype html><head><link rel=stylesheet href=https://blog.autonate.dev/css/main.min.c55e8a3d90783e730dc058804a2726f26d7488162dd2f074752301ca9c1b8657.css></head><html lang=en><head><meta charset=utf-8><meta name=theme-color media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name=mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="black-translucent"><meta name=viewport content="width=device-width,user-scalable=no,initial-scale=1,shrink-to-fit=no,viewport-fit=cover"><meta property="og:url" content="https://blog.autonate.dev/post/discovering-ollama-running-open-source-llms-locally/"><meta property="og:site_name" content="autonate"><meta property="og:title" content="Discovering Ollama: Running Open Source LLMs Locally"><meta property="og:description" content="I recently discovered Ollama (ollama.com), a platform that allows you to run open-source Large Language Models (LLMs) directly on your local machine. As someone with an M1 Pro MacBook Pro, I was intrigued by the idea of leveraging my hardware to run advanced AI models privately. After exploring Ollama and experimenting with open-source models like Llama, Mistral, and Phi, I‚Äôve been amazed at the speed and quality these models deliver‚Äîespecially considering they‚Äôre running locally."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-02-25T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-25T00:00:00+00:00"><meta property="article:tag" content="Ai"><meta property="article:tag" content="Llm"><meta name=twitter:card content="summary"><meta name=twitter:title content="Discovering Ollama: Running Open Source LLMs Locally"><meta name=twitter:description content="I recently discovered Ollama (ollama.com), a platform that allows you to run open-source Large Language Models (LLMs) directly on your local machine. As someone with an M1 Pro MacBook Pro, I was intrigued by the idea of leveraging my hardware to run advanced AI models privately. After exploring Ollama and experimenting with open-source models like Llama, Mistral, and Phi, I‚Äôve been amazed at the speed and quality these models deliver‚Äîespecially considering they‚Äôre running locally."><meta itemprop=name content="Discovering Ollama: Running Open Source LLMs Locally"><meta itemprop=description content="I recently discovered Ollama (ollama.com), a platform that allows you to run open-source Large Language Models (LLMs) directly on your local machine. As someone with an M1 Pro MacBook Pro, I was intrigued by the idea of leveraging my hardware to run advanced AI models privately. After exploring Ollama and experimenting with open-source models like Llama, Mistral, and Phi, I‚Äôve been amazed at the speed and quality these models deliver‚Äîespecially considering they‚Äôre running locally."><meta itemprop=datePublished content="2024-02-25T00:00:00+00:00"><meta itemprop=dateModified content="2024-02-25T00:00:00+00:00"><meta itemprop=wordCount content="689"><meta itemprop=keywords content="Ai,Llm"><title>Discovering Ollama: Running Open Source LLMs Locally |
autonate</title><link rel=apple-touch-icon sizes=180x180 href=/img/favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicons/favicon-16x16.png><link rel="shortcut icon" href=/img/favicons/favicon.ico><meta name=apple-mobile-web-app-title content="autonate"><meta name=application-name content="autonate"><meta name=msapplication-TileColor content="#da532c"><meta name=msapplication-config content="/img/favicons/browserconfig.xml"><meta name=theme-color content="#ffffff"><link rel=preconnect href=https://fonts.googleapis.com><link rel=dns-prefetch href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=https://fonts.gstatic.com><link rel=preconnect href=https://cdn.jsdelivr.net><link rel=dns-prefetch href=https://cdn.jsdelivr.net><link rel=stylesheet href=https://blog.autonate.dev/css/main.min.c55e8a3d90783e730dc058804a2726f26d7488162dd2f074752301ca9c1b8657.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&amp;family=Source+Sans+Pro:wght@400;600;700;900&amp;display=swap"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css><link rel=stylesheet href=/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css><script src=/js/modules/theme.js></script><script src=https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/js/glightbox.min.js></script><script src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script src=https://cdn.jsdelivr.net/npm/dayjs@1.11.13/dayjs.min.js></script><script src=https://cdn.jsdelivr.net/npm/dayjs@1.11.13/locale/en.js></script><script src=https://cdn.jsdelivr.net/npm/dayjs@1.11.13/plugin/relativeTime.js></script><script src=https://cdn.jsdelivr.net/npm/dayjs@1.11.13/plugin/localizedFormat.js></script><script src=https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.js></script><script defer src=/js/post.js></script></head><body><aside aria-label=Sidebar id=sidebar class="d-flex flex-column align-items-end"><header class=profile-wrapper><a href=/ id=avatar class=rounded-circle><img src=/img/commons/nd.jpg width=112 height=112 alt=avatar onerror='this.style.display="none"'></a>
<a class="site-title d-block" href=/>autonate</a><p class="site-subtitle fst-italic mb-0">üëãüèª Hi, I‚Äôm Nathan, a technical professional from the UK with an interest in AI, Automation, System Administration, Cloud Computing, and DevOps</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class=nav><li class=nav-item><a href=/ class=nav-link><i class="fa-fw fas fa-house"></i>
<span>HOME</span></a></li><li class=nav-item><a href=/tags/ class=nav-link><i class="fa-fw fas fa-tags"></i>
<span>TAGS</span></a></li><li class=nav-item><a href=/archives/ class=nav-link><i class="fa-fw fas fa-archive"></i>
<span>ARCHIVES</span></a></li><li class=nav-item><a href=/about/ class=nav-link><i class="fa-fw fas fa-info-circle"></i>
<span>ABOUT</span></a></li></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"><button type=button class="btn btn-link nav-link" aria-label="Switch Mode" id=mode-toggle>
<i class="fas fa-adjust"></i>
</button>
<span class=icon-border></span>
<a href=https://nathandarker.it aria-label=custom target=_blank rel="noopener noreferrer"><i class="fa-solid fa-globe"></i>
</a><a href=https://github.com/aut0nate aria-label=github target=_blank rel="noopener noreferrer"><i class="fab fa-github"></i>
</a><a href=https://www.linkedin.com/in/nathandarker/ aria-label=custom target=_blank rel="noopener noreferrer"><i class="fa-brands fa-linkedin"></i>
</a><a href=https://blog.autonate.dev/index.xml aria-label=rss><i class="fas fa-rss"></i></a></div></aside><div id=main-wrapper class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id=topbar-wrapper class=flex-shrink-0 aria-label="Top Bar"><div id=topbar class="d-flex align-items-center justify-content-between px-lg-3 h-100"><nav id=breadcrumb aria-label=Breadcrumb><span><a href=/>Home</a>
</span><span>Discovering Ollama: Running Open Source LLMs Locally</span></nav><button type=button id=sidebar-trigger class="btn btn-link" aria-label=Sidebar>
<i class="fas fa-bars fa-fw"></i></button><div id=topbar-title>Discovering Ollama: Running Open Source LLMs Locally</div><button type=button id=search-trigger class="btn btn-link" aria-label=Search>
<i class="fas fa-search fa-fw"></i>
</button>
<search id=search class="align-items-center ms-3 ms-lg-0"><i class="fas fa-search fa-fw"></i>
<input class=form-control id=search-input type=search aria-label=search autocomplete=off placeholder=Search...>
</search><button type=button class="btn btn-link text-decoration-none" id=search-cancel>Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class=px-1 data-toc=true><header><h1 data-toc-skip>Discovering Ollama: Running Open Source LLMs Locally</h1><div class="post-meta text-muted"><span>Posted
<time data-ts=1708819200 data-df=ll data-bs-toggle=tooltip data-bs-placement=bottom>%b %e, %Y</time></span><div class="d-flex justify-content-between"><span>By
<em>Nathan</em></span><div><span class=readtime data-bs-toggle=tooltip data-bs-placement=bottom title="789 words"><em>4 min</em> read</span></div></div></div></header><div id=toc-bar class="d-flex align-items-center justify-content-between invisible"><span class="label text-truncate">Discovering Ollama: Running Open Source LLMs Locally</span>
<button type=button class="toc-trigger btn me-1">
<i class="fa-solid fa-list-ul fa-fw"></i></button></div><button id=toc-solo-trigger type=button class="toc-trigger btn btn-outline-secondary btn-sm">
<span class="label ps-2 pe-1">Contents</span>
<i class="fa-solid fa-angle-right fa-fw"></i>
</button>
<dialog id=toc-popup class=p-0><div class="header d-flex flex-row align-items-center justify-content-between"><div class="label text-truncate py-2 ms-4">Discovering Ollama: Running Open Source LLMs Locally</div><button id=toc-popup-close type=button class="btn mx-1 my-1 opacity-75">
<i class="fas fa-close"></i></button></div><div id=toc-popup-content class="px-4 py-3 pb-4"><nav id=TableOfContents><ul><li><a href=#why-run-llms-locally>Why Run LLMs Locally?</a></li><li><a href=#hardware-requirements-for-running-local-llms>Hardware Requirements for Running Local LLMs</a></li><li><a href=#the-hugging-face-community>The Hugging Face Community</a></li><li><a href=#my-experience-with-llama-mistral-and-phi>My Experience with Llama, Mistral, and Phi</a></li><li><a href=#why-ollama-stands-out>Why Ollama Stands Out</a></li><li><a href=#final-thoughts>Final Thoughts</a></li></ul></nav></div></dialog><div class=content><p>I recently discovered <strong>Ollama</strong> (<a href=https://ollama.com/>ollama.com</a>), a platform that allows you to run open-source Large Language Models (LLMs) directly on your local machine. As someone with an M1 Pro MacBook Pro, I was intrigued by the idea of leveraging my hardware to run advanced AI models privately. After exploring Ollama and experimenting with open-source models like <strong>Llama</strong>, <strong>Mistral</strong>, and <strong>Phi</strong>, I‚Äôve been amazed at the speed and quality these models deliver‚Äîespecially considering they‚Äôre running locally.</p><h2 id=why-run-llms-locally id=why-run-llms-locally><span class=me-2>Why Run LLMs Locally?</span>
<a href=#why-run-llms-locally class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Running LLMs locally comes with several benefits, but privacy is the standout advantage. When you use cloud-based AI models, your queries and data are processed on remote servers, raising concerns about data security and privacy. With locally hosted models, everything stays on your device, giving you full control over your inputs and outputs.</p><p>Another reason for the growing interest in local LLMs is the <strong>open-source movement</strong>. The community‚Äôs drive to democratise AI ensures that powerful tools aren‚Äôt locked behind paywalls or controlled by a handful of corporations. Instead, they‚Äôre accessible to developers, researchers, and enthusiasts worldwide, encouraging innovation and collaboration.</p><h2 id=hardware-requirements-for-running-local-llms id=hardware-requirements-for-running-local-llms><span class=me-2>Hardware Requirements for Running Local LLMs</span>
<a href=#hardware-requirements-for-running-local-llms class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Running LLMs locally at an acceptable speed will require high-performing GPUs and plenty of RAM to function efficiently. Here‚Äôs what you will need to consider:</p><ul><li><strong>Model Parameters</strong>: A model‚Äôs parameters are essentially its &ldquo;weights&rdquo;‚Äîthe values it learns during training. Larger models have more parameters, enabling them to handle complex tasks but requiring more computational resources. For example, a 7-billion-parameter model might run smoothly on a modern laptop, while a 65-billion-parameter model would demand a high-end GPU and significant memory.</li><li><strong>Hardware</strong>: While smaller models can run on devices like my M1 Pro MacBook Pro, larger ones need GPUs with high VRAM (e.g., NVIDIA RTX 3090 or better) and sufficient system RAM to store and process the model data efficiently.</li></ul><p>For those with the right hardware, the performance of these models can be incredible, and they are often optimised to maximise efficiency on consumer devices.</p><h2 id=the-hugging-face-community id=the-hugging-face-community><span class=me-2>The Hugging Face Community</span>
<a href=#the-hugging-face-community class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>I also discovered the <a href=https://huggingface.co/>Hugging Face community</a> and found that they play a huge part in the open-source AI revolution. It‚Äôs a hub where developers collaborate on models, datasets, and tools to advance open-source AI. The Hugging Face community has made it easier than ever to discover, fine-tune, and deploy models for various use cases.</p><p>From chatbots to image generation, Hugging Face has become a playground for experimentation and innovation. It‚Äôs exciting to see how people are collaborating globally to create powerful models that rival proprietary ones‚Äîand all while sharing their advancements openly for the benefit of everyone.</p><h2 id=my-experience-with-llama-mistral-and-phi id=my-experience-with-llama-mistral-and-phi><span class=me-2>My Experience with Llama, Mistral, and Phi</span>
<a href=#my-experience-with-llama-mistral-and-phi class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>I‚Äôve been experimenting with <strong>Llama</strong>, <strong>Mistral</strong>, and <strong>Phi</strong> models on my MacBook Pro, and the results have been impressive. The <strong>speed</strong> at which these models perform locally is remarkable, and while they aren‚Äôt quite on par with ChatGPT for general-purpose conversations, they excel in specific use cases.</p><p>For example:</p><ul><li><strong>Llama</strong>: A versatile model that handles a wide range of tasks well.</li><li><strong>Mistral</strong>: Particularly fast and efficient, making it a great choice for devices with limited resources.</li><li><strong>Phi</strong>: Another lightweight option with surprisingly robust capabilities.</li></ul><p>It‚Äôs truly amazing how quickly the open-source community is iterating on these models, making them small and efficient enough to run locally without sacrificing too much performance.</p><h2 id=why-ollama-stands-out id=why-ollama-stands-out><span class=me-2>Why Ollama Stands Out</span>
<a href=#why-ollama-stands-out class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Setting up these models locally might sound daunting, but <strong>Ollama</strong> makes it incredibly easy. The platform provides clear instructions for getting started, and the installation process is smooth and straightforward. Within minutes, I was up and running, testing different models and pushing the limits of my MacBook Pro.</p><h2 id=final-thoughts id=final-thoughts><span class=me-2>Final Thoughts</span>
<a href=#final-thoughts class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>The AI landscape is evolving at a staggering pace, and the open-source movement is a huge part of this revolution. Platforms like Hugging Face and tools like Ollama are empowering individuals to explore the possibilities of AI on their own terms, without relying on cloud services.</p><p>Running LLMs locally is a glimpse into the future of AI‚Äîa future where privacy, accessibility, and community-driven innovation take centre stage. If you‚Äôre curious about exploring this space, I highly recommend checking out <a href=https://ollama.com/>Ollama</a> and diving into the world of open-source LLMs.</p></div><div class="post-tail-wrapper text-muted"><div class=post-tags><i class="fa fa-tags fa-fw me-1"></i>
<a href=/tags/ai/ class="post-tag no-text-decoration">ai</a>
<a href=/tags/llm/ class="post-tag no-text-decoration">llm</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2"><div class=license-wrapper>This post is licensed under <a href=https://creativecommons.org/licenses/by/4.0/>CC BY 4.0</a> by the author.</div></div></div></article></main><aside aria-label=Panel id=panel-wrapper class="col-xl-3 ps-2 text-muted"><div class=access><section id=access-lastmod><h2 class=panel-heading>Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"><a href=/post/my-approach-to-personal-knowledge-management-with-craft/>My Approach to Personal Knowledge Management with Craft</a></li><li class="text-truncate lh-lg"><a href=/post/notebooklm-the-ultimate-learning-assistant/>NotebookLM: The Ultimate Learning Assistant</a></li><li class="text-truncate lh-lg"><a href=/post/moving-from-portainer-to-dockge/>Moving from Portainer to Dockge</a></li><li class="text-truncate lh-lg"><a href=/post/glance-my-favourite-homelab-dashboard/>Glance: My Favourite Homelab Dashboard</a></li><li class="text-truncate lh-lg"><a href=/post/discovering-n8n-the-workflow-automation-platform/>Discovering n8n: The Workflow Automation Platform</a></li></ul></section><section><h2 class=panel-heading>Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"><a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/ai/>ai</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/api/>api</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/apple/>apple</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/automation/>automation</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/blog/>blog</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/chatgpt/>chatgpt</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/cli/>cli</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/containers/>containers</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/craft/>craft</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/devops/>devops</a></div></section></div><div class="toc-border-cover z-3"></div><section id=toc-wrapper class="invisible position-sticky ps-0 pe-4 pb-4"><h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2><nav id=toc></nav></section></aside></div><div class=row><div id=tail-wrapper class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id=related-posts aria-labelledby=related-label><h3 class=mb-4 id=related-label>Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class=col><a href=/post/first-impressions-of-gpt-4/ class="post-preview card h-100"><div class=card-body><time data-ts=1681430400 data-df=ll>%b %e, %Y</time><h4 class="pt-0 my-2">First Impressions of GPT-4</h4><div class=text-muted><p>It‚Äôs only been a few weeks since OpenAI released GPT-4, and I‚Äôve already been putting it to work. As a ChatGPT Plus subscriber, I‚Äôve gained access to this new model, albeit with frustratingly low rate ‚Ä¶</p></div></div></a></article><article class=col><a href=/post/the-ai-explosion-how-its-changing-the-way-we-work-and-live/ class="post-preview card h-100"><div class=card-body><time data-ts=1696550400 data-df=ll>%b %e, %Y</time><h4 class="pt-0 my-2">The AI Explosion: How It‚Äôs Changing the Way We Work and Live</h4><div class=text-muted><p>We‚Äôre living in the middle of an AI revolution. The rapid advancements in artificial intelligence feel like the dawn of a new era‚Äîmuch like how the internet and the World Wide Web transformed the way ‚Ä¶</p></div></div></a></article><article class=col><a href=/post/understanding-prompt-engineering-the-art-of-communicating-with-llms/ class="post-preview card h-100"><div class=card-body><time data-ts=1702598400 data-df=ll>%b %e, %Y</time><h4 class="pt-0 my-2">Understanding Prompt Engineering: The Art of Communicating with LLMs</h4><div class=text-muted><p>Prompt engineering is a crucial skill in today‚Äôs AI-driven world. At its core, it‚Äôs the process of designing and optimising prompts to interact effectively with natural language processing (NLP) ‚Ä¶</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"><a href=/post/understanding-prompt-engineering-the-art-of-communicating-with-llms/ class="btn btn-outline-primary" aria-label=Older><p>Understanding Prompt Engineering: The Art of Communicating with LLMs</p></a><a href=/post/getting-started-with-gpts-how-i-built-a-custom-assistant-to-learn-linux/ class="btn btn-outline-primary" aria-label=Newer><p>Getting Started with GPTs: How I Built a Custom Assistant to Learn Linux</p></a></nav><footer aria-label="Site Info" class="d-flex flex-column justify-content-center text-muted
flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3"><p>¬©
<time>2026</time>
<a href=https://github.com/aut0nate>aut0nate</a>.
<span data-bs-toggle=tooltip data-bs-placement=top title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p><p>Using the <a data-bs-toggle=tooltip data-bs-placement=top title=v1.0.2 href=https://github.com/geekifan/hugo-theme-chirpy target=_blank rel=noopener>Chirpy</a> theme for <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a>.</p></footer></div></div><div id=search-result-wrapper class="d-flex justify-content-center d-none"><div class="col-11 content"><div id=search-hints><section><h2 class=panel-heading>Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"><a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/ai/>ai</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/api/>api</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/apple/>apple</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/automation/>automation</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/blog/>blog</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/chatgpt/>chatgpt</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/cli/>cli</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/containers/>containers</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/craft/>craft</a>
<a class="post-tag btn btn-outline-primary" href=https://blog.autonate.dev/tags/devops/>devops</a></div></section></div><div id=search-results class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div><script>const DEFAULT_CONFIG={search:{minChars:1,maxResults:5,fields:{title:!0,description:!0,section:!0,contents:!0},strictMode:!0}};class FastSearch{constructor({searchInput:e,resultsContainer:t,json:n,searchResultTemplate:s=null,noResultsText:o=null}){this.searchInput=e,this.resultsContainer=t,this.json=n,this.searchResultTemplate=s,this.noResultsText=o,this.init()}init(){this.loadSearchIndex(),this.searchInput.addEventListener("input",e=>{if(!this.searchIndex){this.resultsContainer.innerHTML='<li class="search-message">Loading search index...</li>';return}this.performSearch(this.searchInput.value)})}async loadSearchIndex(){try{const e=await fetch(this.json);if(!e.ok)throw new Error("Failed to load search index");const t=await e.json();this.searchIndex=t.map(e=>({...e,searchableTitle:e.title?.toLowerCase()||"",searchableDesc:e.desc?.toLowerCase()||"",searchableSection:e.section?.toLowerCase()||"",searchableContents:e.contents?.toLowerCase()||""}))}catch(e){console.error("Error loading search index:",e),this.resultsContainer.innerHTML='<li class="search-message">Error loading search index...</li>'}}escapeHtml(e){return e?e.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/>/g,"&gt;").replace(/"/g,"&quot;").replace(/'/g,"&#039;"):""}containsTerm(e,t){return!!e&&!!t&&e.includes(t)}performSearch(e){if(e=e.toLowerCase().trim(),!e||!this.searchIndex||e.length<DEFAULT_CONFIG.search.minChars){this.resultsContainer.innerHTML="";return}const s=[e,...e.split(/\s+/).filter(e=>e.length>0)],t=[...new Set(s)],o=DEFAULT_CONFIG.search.strictMode,n=this.searchIndex.map(e=>{const s=this.checkFieldsForMatch(e,t[0]);if(o&&!s)return{item:e,score:0,matched:!1};let n=0,i=0,a=!1;t.forEach((t,s)=>{const r=s===0,o=this.checkFieldsForMatch(e,t);o&&(i++,o.inTitle&&(n+=r?10:5,a=!0),o.inDesc&&(n+=r?8:4),o.inSection&&(n+=r?6:3),o.inContents&&(n+=r?4:2))});const r=i/t.length,c=n*r*(a?1.5:1);return{item:e,score:c,matched:s}}).filter(e=>e.matched).sort((e,t)=>t.score-e.score).slice(0,DEFAULT_CONFIG.search.maxResults).map(e=>e.item);if(n.length===0){this.resultsContainer.innerHTML='<p class="mt-5">Oops! No results found.</p>';return}const i=n.map(e=>{let t="",n="";return e.categories&&(t=e.categories.join(", "),t=`<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${t}</div>`),e.tags&&(n=e.tags.join(", "),n=`<div><i class="fa fa-tag fa-fw"></i>${n}</div>`),`
        <article class="px-1 px-sm-2 px-lg-4 px-xl-0">
          <header>
            <h2><a href="${this.escapeHtml(e.permalink)}">${this.escapeHtml(e.title)}</a></h2>
            <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">
              ${t}
              ${n}
            </div>
          </header>
          <p>${this.escapeHtml(e.contents)}</p>
        </article>
      `}).join("");this.resultsContainer.innerHTML=i}checkFieldsForMatch(e,t){const n={inTitle:!1,inDesc:!1,inSection:!1,inContents:!1};return DEFAULT_CONFIG.search.fields.title&&this.containsTerm(e.searchableTitle,t)&&(n.inTitle=!0),DEFAULT_CONFIG.search.fields.description&&this.containsTerm(e.searchableDesc,t)&&(n.inDesc=!0),DEFAULT_CONFIG.search.fields.section&&this.containsTerm(e.searchableSection,t)&&(n.inSection=!0),DEFAULT_CONFIG.search.fields.contents&&this.containsTerm(e.searchableContents,t)&&(n.inContents=!0),!!(n.inTitle||n.inDesc||n.inSection||n.inContents)&&n}}const search=new FastSearch({searchInput:document.getElementById("search-input"),resultsContainer:document.getElementById("search-results"),json:`/index.json`})</script></div><aside aria-label="Scroll to Top"><button id=back-to-top type=button class="btn btn-lg btn-box-shadow">
<i class="fas fa-angle-up"></i></button></aside></div><div id=mask class="d-none position-fixed w-100 h-100 z-1"></div></body></html>