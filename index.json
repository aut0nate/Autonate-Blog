[{"categories":null,"contents":"As a technical professional and somebody who is constantly learning new skills, I’ve always taken time to document what I learn. I learned early on that relying on memory simply doesn’t scale, and documentation isn’t an afterthought for me, it’s a deliberate part of my workflow.\nThere are two main reasons I document things:\nTo help future me remember how I did something To share knowledge with others If you’ve worked in IT for any length of time, you’ll know how fast things move. A tool that feels essential today can be quietly deprecated tomorrow. In the age of AI, that pace has only accelerated. New frameworks, platforms, services, and workflows appear constantly, and the number of rabbit holes you can go down in any given tech stack is endless.\nAt some point, it became clear that trying to keep everything in my head simply wasn’t sustainable.\nWhy Documentation Became Non-Negotiable for Me For a long time, my “system” consisted of browser bookmarks, half-written notes, and random snippets saved wherever felt convenient at the time. When I needed to recall something, I’d spend far too long searching for it, usually under pressure. That friction added stress and anxiety, especially when I needed to act quickly.\nThe cognitive burden of trying to remember everything is real. Once I accepted that, things changed.\nI slowed down and started documenting properly. Over time, that documentation evolved into a personal tech wiki. It’s easily one of the best investments of time I’ve made in my career.\nThe benefits were immediate and long-lasting:\nFaster recall when configuring or troubleshooting systems Better retention through the act of writing things down Lower stress, because I wasn’t relying on memory alone Documentation became the one habit I’ve been consistent with over the years, and it’s been the bedrock of how I learn and work.\nThe Long Road to Finding the Right Tool I’ve tried a lot of documentation tools over the years. Plain Markdown files, wikis, note apps, and various “all-in-one” productivity tools. Most of them were fine, but none really stuck.\nAround three years ago, I settled on Craft, and it’s been my system of choice ever since. It didn’t just replace my notes. It became the place where my knowledge actually lives.\nCraft strikes a balance that I hadn’t found elsewhere. It’s structured without being rigid, powerful without feeling heavy, and flexible enough to grow with how I think and work.\nWhat is Craft? At its core, Craft is a document creation and knowledge management platform designed for both individuals and teams. It runs on iOS, macOS, Windows, and the web, which means my notes are always with me, regardless of device.\nWhat really sets Craft apart is how it encourages structure without forcing it. Documents are block-based, which makes it easy to organise thoughts, break down complex topics, and link related ideas together over time.\nAI That’s Actually Useful Craft includes an AI Assistant that’s deeply integrated into the workspace. I use it for:\nSummarising long documents Refining notes when I want to share them publicly Finding information across my knowledge base without exact keywords Because the Assistant understands the context of my documents, tasks, and links, it feels less like a bolt-on feature and more like a natural extension of the system. Knowing that Craft plans to make this assistant more agentic over time is genuinely exciting.\nRich Documents, Not Just Notes My documentation often includes more than text. Craft makes it easy to embed:\nImages and diagrams Code snippets and tables Videos, audio, and files That flexibility means my tech wiki can capture real-world context, not just commands or configuration steps. I also like that documents can be styled, themed, and even published as websites when I want to share something publicly. Check out the Collection for an example of this.\nTasks and Daily Notes in One Place One unexpected win with Craft has been task management. I can capture tasks directly inside notes, send them to a central Inbox, and manage due dates and reminders without switching tools.\nDaily Notes have become part of my routine. I use them to plan my day, track what I worked on, and reflect at the end of the week. Having that alongside my documentation keeps everything connected.\nBuilt for Collaboration, Even If You Mostly Work Solo Even though I use Craft for personal knowledge management, the collaboration features are solid:\nReal-time editing Comments and version history Granular sharing permissions When I do need to collaborate or share knowledge with others, I don’t need to migrate content elsewhere.\nCustomisation, Integrations, and MCP Craft supports Markdown, keyboard shortcuts, templates, and API access. More recently, Craft introduced support for the Model Context Protocol (MCP), which takes integrations a step further by allowing Craft to connect directly with third-party tools and services.\nThis opens up some genuinely powerful workflows. Craft can now act as a live context layer for your tools, not just a place where information is stored. Whether that’s querying external systems, enriching documents with real-time data, or using Craft as a central memory for AI-driven workflows, MCP makes those ideas practical rather than theoretical.\nCombined with features like backlinks, AI-powered search, offline access, and flexible export formats, my data never feels locked in. Craft increasingly feels like the connective tissue between how I think, document, and work.\nLessons Learned Along the Way The biggest lesson for me has been that tools matter, but habits matter more. Craft didn’t magically fix my documentation problems. What it did was remove friction, which made it easier to be consistent.\nOnce documenting became the default rather than an afterthought, everything else followed.\nFinal Thoughts Building and maintaining a personal tech wiki has been one of the most valuable things I’ve done as a technical professional. It’s reduced stress, improved recall, and given me confidence that I can always pick something back up, even years later.\nCraft happens to be the tool that finally made that sustainable for me. It fits how I think, how I learn, and how I work. If documentation has ever felt like a chore or something you’ll “get to later”, I’d encourage you to rethink it.\nFuture you will be grateful you did.\n","permalink":"https://blog.autonate.dev/post/my-approach-to-personal-knowledge-management-with-craft/","tags":["craft","productivity"],"title":"My Approach to Personal Knowledge Management with Craft"},{"categories":null,"contents":"In the fast-evolving world of technology, staying ahead means constantly learning new skills, whether that’s mastering Linux, diving into Docker, or exploring a new cloud service. That hasn’t changed. Reading documentation, exploring blog posts, and watching tutorials are still essential parts of the learning process. What has changed for me is how I bring all of that material together. The tool I use to do that is NotebookLM which has slotted neatly into my learning toolkit, helping me research, retain, and make sense of the information.\nIn my opinion, NotebookLM is genuinely one of the best AI tools on the market for learning new skills and one of Google\u0026rsquo;s best products, it really is that good. It provides such an engaging way to learn and retain information and everything is firmly grounded in the sources you provide to it.\nThe Power of Grounded Learning NotebookLM, powered by Google Gemini, functions less like a generic chatbot and more like an intelligent AI research assistant. Its genius lies in grounding, it restricts its answers to the sources you provide, such as documents, audio, or websites. This ensures the output is directly relevant and tailored to the subject you are learning about. Ultimately, it transforms static reference materials into an active thinking partner that you can converse with.\nRegardless of the subject, you can upload your own documentation or reference sources on the web, and NotebookLM will ensure that the output is directly relevant to your sources. This grounding is what makes it so useful, however, you should ensure that the source material is of a high quality in order to obtain the best outputs.\nMy Workflow: Learning New Technical Subjects When I tackle a new technical subject, my process is now built entirely around NotebookLM\nI create a new Notebook and upload my sources, these sources can be anything from PDF\u0026rsquo;s to websites.\nI then start asking questions about the sources, this helps me quickly zero in on key concepts and extract relevant information efficiently.\nNext, I leverage the incredible multimodal features to consume the knowledge in different ways, dramatically boosting retention.\nTurning Notes into Narratives: Audio and Video Overviews The evolution of how NotebookLM handles source material is where it truly shines, particularly for someone who prefers auditory or visual learning.\nThe Audio Overviews feature, which arrived in late 2024, converts lengthy documents into a conversational, podcast-like discussion between two AI hosts. This functionality is fantastic for condensing complex or lengthy documents into accessible audio summaries that I can listen to while on the go. They even introduced interactive Audio Overviews, allowing you to participate in the discussion by asking questions of the AI hosts.\nMore recently, I have started to use the Video Overview feature, which is an excellent idea. This functionality, introduced in 2025, transforms document summaries into visual slide-style videos, complete with AI narration, diagrams, and structured explanations. I find it extremely useful for grasping complex relationships between concepts.\nEnhancing Retention: Flashcards, Quizzes, and Mind Maps To ensure that the information sticks, I move from consumption to active recall.\nWhile NotebookLM gives you structured explanations, the process of turning those explanations into usable study aids is seamless. I take advantage of flash cards and quizzes to test myself on the key findings and concepts extracted from my source material. For visual learners, new features like Infographics and Slide Decks allow you to visualise your source material, which acts similarly to a mind map by mapping out the relationships between different facts.\nThis multi-faceted approach to learning and consuming content, asking targeted questions, listening to a podcast form discussion, watching a visual video overview, and then testing myself—has been a great tool in terms of knowledge retention and has definitely helped me learn more efficiently and effectively.\nIf you want to see a real example of how I use NotebookLM in practice, click on the following link: Explore my NotebookLM example\nWho is NotebookLM For? While I use it primarily for technical learning, the tool has broad applicability. Initially, it was designed for researchers, but it has quickly been adopted by students and companies. For anyone who needs to quickly synthesise a large volume of source material, be it for academic work, professional documentation, or personal curiosity—NotebookLM has you covered.\nIt is truly like having a dedicated research assistant that handles the complex information analysis, allowing you to focus on learning and applying the insights.\nFinal Thoughts For a long time, I assumed effective learning meant pushing through long documents and hoping the important bits would stick. That work is still necessary, but NotebookLM has changed how I work with that material. By keeping every answer grounded in my own sources, it adds a layer of trust and structure that makes the learning process feel far more intentional.\nIf you’re struggling to keep up with the pace of technological change or finding it hard to organise your research, NotebookLM is well worth exploring. It helps turn collections of documentation into something you can actively engage with, making learning feel less like passive consumption and more like a process you’re in control of.\n","permalink":"https://blog.autonate.dev/post/notebooklm-the-ultimate-learning-assistant/","tags":["ai","llm","notebooklm","google","gemini"],"title":"NotebookLM: The Ultimate Learning Assistant"},{"categories":null,"contents":"I first dived into containerisation around 2020, eventually settling on a Virtual Private Server (VPS) setup where the focus shifted from managing hardware to mastering software and services. For a long time, the backbone of that software setup was Portainer. Portainer served as the graphical interface for managing the 20+ services I run in stacks on my VPS, handling everything from my reverse proxy to monitoring tools.\nPortainer is a robust GUI for managing containers and stacks, and I want to make it clear that my migration wasn\u0026rsquo;t driven by any failing on its part; it’s an excellent piece of kit. However, as my homelab evolved, I started looking for something that offered an even simpler and easier management experience. I wanted a dedicated tool that stripped back the complexity, allowing me to focus entirely on tweaking my Docker Compose files and stacks. That search led me straight to Dockge.\nDiscovering Dockge The appeal of Dockge was immediate. It met my need for a lighter touch in stack management and quickly proved to be a powerful tool with some genuinely useful features, such as:\nComprehensive Stack Management: Dockge provides tools to manage your compose.yaml files, allowing you to create, edit, start, stop, restart, delete, and update Docker images.\nInteractive Editor and Terminal: It features an interactive editor for docker-compose.yaml files and includes an interactive web terminal for stack interaction.\nReactive and Real-time: The interface is reactive, meaning progress (such as pulling images or deploying stacks) and terminal output are displayed in real-time.\nMultiple Agent Support: Dockge supports multiple agents, enabling you to manage stacks located on different Docker hosts within a single unified interface.\nMoving between Docker management tools is often straightforward because of the fundamental concept of container portability. Whether you\u0026rsquo;re moving to a new server or switching GUIs, the ability to copy a compose file and run docker compose up -d makes transitions far smoother. Dockge allows me to manage the stacks containing my various services like my automation hub, n8n, and my RSS reader, Miniflux - with the simplicity I was seeking.\nA Familiar Face for Homelabbers If you, use Uptime Kuma to monitor the status of your services, you will instantly feel at home using Dockge because they are both created by the same developer, Louis Lam. The clean, intuitive user interface shares the same aesthetic sensibilities as Uptime Kuma, making the learning curve negligible\nDockge focuses on the core task of managing Docker Compose files, allowing you to edit, deploy, and monitor your stacks with minimal fuss. For those who appreciate simplicity and efficiency in their homelab, this tool is well worth exploring, and thank you Louis for developing and maintaining these excellent tools!\nGetting Started If you’re running services in a Docker environment and are looking for a streamlined, clean interface to manage your stacks, I highly recommend checking out Dockge.\nFeel free to use the following docker-compose.yaml file and amend the relevant values as needed:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 services: dockge: image: louislam/dockge:latest container_name: dockge restart: unless-stopped volumes: - /var/run/docker.sock:/var/run/docker.sock - ./data:/app/data - /opt/stacks:/opt/stacks networks: - mgmt-net ports: - \u0026#34;5001:5001\u0026#34; security_opt: - no-new-privileges:true logging: options: max-size: \u0026#34;10m\u0026#34; max-file: \u0026#34;3\u0026#34; environment: DOCKGE_STACKS_DIR: /opt/stacks TZ: Europe/London DOCKGE_ENABLE_CONSOLE: \u0026#34;true\u0026#34; labels: com.centurylinklabs.watchtower.enable: \u0026#34;false\u0026#34; networks: mgmt-net: external: true Final Thoughts Making the switch from Portainer to Dockge was a decision rooted in seeking simplification, which aligns well with my current homelab philosophy of focusing on the software rather than getting overwhelmed by the architecture. While Portainer is undeniably feature-rich, Dockge provides the focused, easy-to-manage experience I was looking for.\nIt’s a perfect reminder that the beauty of homelabs lies in their flexibility and you should always choose the tools that work best for you, whether that means running a complex setup or opting for elegant simplicity. If you\u0026rsquo;re managing containers and value an easy-to-use interface, give Dockge a look and see how it fits into your workflow.\n","permalink":"https://blog.autonate.dev/post/moving-from-portainer-to-dockge/","tags":["homelab","docker","containers"],"title":"Moving from Portainer to Dockge"},{"categories":null,"contents":"I recently started using Glance as the primary dashboard for my homelab, and it has quickly become one of my most-used tools. Glance allows me to monitor all my Docker containers, network devices, and services at a glance (pun intended), all within a clean, minimalist interface. It’s lightweight, fast, and extremely customisable, giving me real-time visibility into my entire environment.\nWhat Is Glance? Glance is a self-hosted dashboard that you can configure entirely through a simple YAML file. It’s designed for speed, clarity, and ease of maintenance while giving you total control over layout and content. You can display container statuses, system metrics, bookmarks, RSS feeds, or even pull live data from APIs. It offers a clean, centralised view. of the information that matters most, keeping everything organised and accessible.\nThe built-in widgets are well-designed and easy to set up, requiring only a few lines of YAML to get started. You can find a full list of widgets and configuration options here: Glance Widget Configuration Guide\nFor those who want to go beyond the defaults, the Glance community offers a fantastic library of custom widgets that expand its functionality even further. From monitoring external APIs to visualising network data, these widgets allow for deep personalisation: Community Widgets\nGlance’s YAML-driven design makes it extremely portable. You can easily back up, share, or version-control your entire configuration with ease.\nMy Setup In my homelab, I use Glance as both a monitoring dashboard and a quick-access portal to essential services. The combination of built-in and community widgets gives me a tailored experience that suits my daily workflow. Here are some of the custom widgets I’m currently using:\nTailscale Devices – Displays all devices connected to my Tailscale network. Cloudflare Tunnels – Shows the status of my active Cloudflare tunnels. NextDNS Stats – Provides DNS-level analytics and insights from NextDNS. GitHub Personal Repos – Displays my GitHub repositories and their most recent updates. Raindrop Latest Links – Shows the latest bookmarks I’ve added in Raindrop.io. Time Bar – Adds a clean, visual representation of the day’s progress across the top of my dashboard. Configuration is simple; you define each widget in your glance.yml file, specify its type and parameters, and organise it into columns and pages. Before diving in, I highly recommend reading up on how Glance structures layouts using pages and columns, as this directly affects how your widgets are displayed: Pages \u0026amp; Columns Explained\nSome widgets require API keys, tokens, or service endpoints. These can be obtained from the respective services, but remember to keep your credentials safe and avoid sharing them publicly. If your configuration includes sensitive information such as API keys or tokens, you should redact this before sharing.\nMy Dashboard Layout It took me a couple of days to refine my layout, balancing functionality with aesthetics. My top section is dedicated to live monitoring, things like websites and container statuses, network devices, and DNS analytics. Below that, I keep convenience widgets such as bookmarks and GitHub repos. The result is a clean, structured dashboard that provides instant visibility into my services without unnecessary clutter.\nHere’s a look at my current setup:\nOne of Glance’s standout features is that it’s mobile-friendly. Whether I’m at home or on the move, I can quickly pull up my dashboard on my phone to check that all services are running smoothly. It’s fully responsive, so nothing feels cramped or broken on smaller screens.\nI also appreciate just how lightweight Glance is. It consumes minimal resources, so it runs quietly in the background without slowing down the server.\nWhy Glance Works So Well Glance fits seamlessly into my broader homelab ecosystem and serves as the entry point into my services. Although my Glance instance is technically public-facing, it’s protected behind Authentik, ensuring secure authentication and controlled access. This setup allows me to enjoy the convenience of remote visibility while maintaining strong security boundaries.\nAnother major advantage of Glance is how quickly you can make adjustments. Adding, removing, or reorganising widgets takes just a few minutes, making it easy to experiment with new layouts or test different configurations. The YAML format keeps everything readable and version-controlled, allowing for safe rollbacks and iterative improvements. Whether I’m testing a new widget from the community or reorganising columns to improve readability, the process is smooth and predictable.\nFinal Thoughts After experimenting with a range of dashboards, Glance is the one I’ve settled on. It’s light, fast, and flexible, offering just the right amount of customisation without complexity. It integrates seamlessly with my existing services, stays secure behind Authentik, and provides a central view of my services.\nAs I continue refining my homelab, I plan to explore more of what Glance can do, especially as new widgets and integrations become available. There are a few on my radar already, and I’m interested in experimenting with community-created options that connect to my other tools. The ability to adapt and expand my dashboard over time makes Glance an evolving part of my workflow.\nIf you run a homelab or manage multiple self-hosted services, I highly recommend giving Glance a try. Whether you want a minimalist overview or a detailed control centre, it’s an elegant, efficient solution that’s easy to maintain and a pleasure to use.\n","permalink":"https://blog.autonate.dev/post/glance-my-favourite-homelab-dashboard/","tags":["homelab"],"title":"Glance: My Favourite Homelab Dashboard"},{"categories":null,"contents":"Automation has always fascinated me for its ability to simplify complex processes and free up time for creative work. That curiosity naturally led me to explore new tools and that’s how I discovered n8n, a workflow automation platform that has quickly become one of my favourite tools.\nI kept seeing videos and articles about n8n pop up and it was one of those tools that never quite left my “must-try” list. As someone who’s passionate about automation, I was curious to see what made it stand out. Once I started digging deeper, I quickly realised how powerful and flexible it was.\nWhat is n8n? n8n (short for “nodemation” and pronounced n-eight-n) is a workflow automation platform that lets you connect apps, services, and APIs without writing complex code. Think of it as a self-hosted alternative to Zapier or Make but with far greater flexibility and control.\nIt uses nodes to represent actions or data transformations, and you can chain these nodes together in workflows to automate almost anything - from sending emails and integrating APIs to building AI-powered data agents.\nBest of all, you can self-host n8n, giving you full control over your data and privacy, something that really resonated with me.\nSetting It Up in My Homelab Before diving in, there are a few prerequisites: a running Docker environment (or Portainer if you prefer a GUI), a domain managed through a provider like Cloudflare, and some basic knowledge of reverse proxying with tools such Nginx Proxy Manager. Once those are set up, the rest is easy.\nWith plenty of spare resources in my homelab, running n8n on my VPS was an obvious choice. I deployed it via Portainer and tweaked the stack configuration to suit my setup.\nHere is my docker-compose.yaml file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 services: n8n: image: docker.n8n.io/n8nio/n8n:latest container_name: n8n expose: - \u0026#34;5678\u0026#34; restart: unless-stopped user: \u0026#34;1000:1000\u0026#34; environment: # General settings - TZ=Europe/London - N8N_HOST=${SUBDOMAIN}.${DOMAIN_NAME} # e.g., n8n.example.com - N8N_PORT=5678 - N8N_PROTOCOL=https - NODE_ENV=production - WEBHOOK_URL=https://${SUBDOMAIN}.${DOMAIN_NAME}/ - N8N_RUNNERS_ENABLED=true - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true # Database - DB_TYPE=postgresdb - DB_POSTGRESDB_HOST=n8n-db - DB_POSTGRESDB_PORT=5432 - DB_POSTGRESDB_USER=${N8N_DB_USER} - DB_POSTGRESDB_PASSWORD=${N8N_DB_PASSWORD} - DB_POSTGRESDB_DATABASE=${N8N_DB_NAME} # Security / Privacy - N8N_BLOCK_ENV_ACCESS_IN_NODE=false - N8N_DIAGNOSTICS_ENABLED=false - N8N_VERSION_NOTIFICATIONS_ENABLED=false - N8N_TRUSTED_PROXIES=${N8N_TRUSTED_PROXIES} volumes: - n8n_data:/home/node/.n8n - ./local-files:/files networks: - edge-net - n8n-backend depends_on: n8n-db: condition: service_healthy security_opt: - no-new-privileges:true read_only: true tmpfs: - /tmp - /home/node/.cache:uid=1000,gid=1000,mode=770 logging: options: max-size: \u0026#34;10m\u0026#34; max-file: \u0026#34;3\u0026#34; n8n-db: image: postgres:15-alpine container_name: n8n-db restart: unless-stopped environment: - TZ=Europe/London - POSTGRES_USER=${N8N_DB_USER} - POSTGRES_PASSWORD=${N8N_DB_PASSWORD} - POSTGRES_DB=${N8N_DB_NAME} volumes: - n8n-db:/var/lib/postgresql/data networks: - n8n-backend healthcheck: test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;pg_isready -U ${N8N_DB_USER} -h localhost\u0026#34;] interval: 10s timeout: 5s retries: 5 start_period: 30s security_opt: - no-new-privileges:true logging: options: max-size: \u0026#34;10m\u0026#34; max-file: \u0026#34;3\u0026#34; volumes: n8n_data: n8n-db: networks: edge-net: external: true n8n-backend: driver: bridge Here is my .env file:\n1 2 3 4 5 6 DOMAIN_NAME=example.com SUBDOMAIN=n8n N8N_DB_USER=\u0026lt;n8n_db_user\u0026gt; N8N_DB_PASSWORD=\u0026lt;n8n_db_password\u0026gt; N8N_DB_NAME=n8n N8N_TRUSTED_PROXIES=127.0.0.1 After confirming the stack was running, I configured my Cloudflare Tunnel and linked my subdomain through Nginx Proxy Manager for external access. Then I created the owner account, registered for the free license key, and activated my instance - simple and seamless.\nNote: If you want to self-host n8n, check out the official installation guide.\nLearning by Doing n8n acts as a bridge between systems that were never designed to talk to each other, a simple yet powerful way to connect APIs, webhooks, and apps into one cohesive ecosystem.\nI learn best by doing, so I watched a few YouTube tutorials to understand the basics: workflows, nodes, and credentials. The official n8n Tutorials playlist is a great place to start: YouTube.\nIt only took a few days to feel comfortable, and once I understood the fundamentals, everything clicked. The visual interface makes experimentation easy, while the modular design means you can start small and gradually build more complex automations at your own pace.\nFrom Linear Workflows to AI-Powered Agents The feature that really caught my attention was AI Agents. This isn’t your standard “if-this-then-that” automation - AI Agents bring context and memory into workflows. Instead of following a rigid sequence of actions, they can make decisions based on previous interactions.\nAs someone deeply invested in AI, this immediately stood out. You can connect to major LLM providers such as OpenAI, Google, or my personal favourite, OpenRouter, for its wide range of models and flexibility. Setting it up is as easy as adding your OpenRouter API key in the credentials section.\nFinal Thoughts Exploring n8n has been very rewarding and it reminded me how powerful automation tools can be when paired with curiosity and experimentation. If you’re considering trying it, start small, connect two tools you already use and see what happens. You’ll be amazed at how quickly ideas evolve once you see what’s possible.\nAfter just a few weeks, n8n has become a cornerstone of my automation workflow stack. It’s flexible, powerful, and endlessly extendable and best of all, it’s free and self-hostable.\nIf you’re passionate about automation, have a homelab, or simply want to connect your favourite tools in smarter ways, n8n is absolutely worth your time. I’ll be sharing more about the workflows I’m building soon, including how I’m using AI Agents to create dynamic, context-aware automations.\n","permalink":"https://blog.autonate.dev/post/discovering-n8n-the-workflow-automation-platform/","tags":["homelab","automation"],"title":"Discovering n8n: The Workflow Automation Platform"},{"categories":null,"contents":"When I first started exploring the world of homelabs, I thought it had to involve racks of hardware, blinking lights, and endless tinkering. I even wrote about my early experiments here. At that time, I repurposed an old desktop PC and dived head-first into virtualisation with Proxmox, containers, and self-hosted services.\nIt was fun, but I quickly realised that what really interested me wasn’t the hardware - it was the software and services. I loved spinning up tools, learning how they worked, and making them accessible on my own terms. The hardware side, while interesting, left me feeling stuck in “decision paralysis”, what’s the perfect setup? What hardware should I buy next? Should I build a Kubernetes cluster? Instead of enjoying the process, I was constantly second-guessing.\nSo I decided to simplify my approach. I went down the Virtual Private Server (VPS) path, and I haven’t looked back since.\nWhy a VPS? The homelab subreddit is full of impressive setups, dedicated racks, enterprise gear, and serious investment. I enjoy seeing those, but for me, a low-cost VPS was the right middle ground.\nHere’s why:\nNo worries about hardware failures or power bills.\nEasy to scale resources as needed.\nAlways online and accessible, without poking holes in my home network.\nFulfils my core goals when I started this journey of learning more about Linux and Docker\nAfter looking around, I landed on Contabo. Their VPS plans are very affordable and, so far, rock-solid in performance. I started with a fresh Ubuntu install and built my cloud lab from there.\nThe Core Stack On my VPS, the host is running a few key services that everything else builds on:\nDocker – Containers are the backbone of my setup. I run 20+ services in stacks, all managed via Portainer.\nTailscale – Provides secure private access to services I don’t want on the public internet.\nCloudflare Tunnels – Handles exposure of public services without needing to open ports or worry about DDoS protection.\nThis trio of Docker, Tailscale, and Cloudflare Tunnels is what makes the whole setup simple, secure, and scalable.\nPrivate Services (Tailnet Only) Some services don’t need to be on the public internet. These run exclusively inside my Tailnet:\nNginx Proxy Manager – SSL termination and reverse proxying.\nPortainer – GUI for managing containers and stacks.\nUptime Kuma – Uptime monitoring for my services.\nFile Browser – Quick access to files on my VPS via a GUI.\nWith Tailscale, I can access these from anywhere without exposing them. No port forwarding and no firewall headaches - it just works!\nPublic Services (Through Cloudflare + Nginx Proxy Manager) For services I want to access outside my Tailnet, I combine Nginx Proxy Manager (NPM) with Cloudflare Tunnels.\nNPM handles SSL termination and routes requests to the right backend containers. Cloudflare gives me a secure entry point into my VPS, without exposing my IP directly.\nSome examples of my public-facing services:\nn8n – My automation hub.\nMiniflux – My RSS reader.\nIT Tools – A handy collection of web utilities.\nI’ve also added Authentik as my identity provider, replacing basic auth with proper single sign-on. For example, Miniflux now sits neatly behind Authentik, which feels both professional and secure.\nMy Lab Here are some screenshots which provide an insight into my setup:\nPortainer:\nCloudflare Tunnel:\nNginx Proxy Manager:\nLessons Learned Running a VPS homelab has taught me a few important lessons:\nStart small. Don’t get overwhelmed with hardware or architecture decisions. Just spin up a service and learn from there.\nThings will break. Sometimes you just need to step away, sleep on it, and return with fresh eyes.\nHomelabs are personal. There’s no single “correct” setup. What matters is what works for you.\nAnd while I’ve gone the VPS route, I still have my original machine humming away under my desk. These days, it runs a mix of Linux and Windows VMs, including a small domain controller tied into Azure which is mainly for work testing.\nFinal Thoughts I may revisit a full hardware-based homelab one day, but for now, my VPS with Contabo strikes the right balance. It gives me the freedom to explore, learn, and self-host without the overhead of managing hardware.\nContabo has been solid so far, and I wouldn’t hesitate to recommend them if you’re curious about trying the VPS route yourself.\nThe beauty of homelabs is their flexibility, they can be as simple or as complex as you want. Mine lives on a VPS, yours might live in a rack in your garage, and both are equally valid.\nAt the end of the day, it’s about learning, experimenting, and having fun with technology.\n","permalink":"https://blog.autonate.dev/post/my-cloud-homelab-setup/","tags":["homelab"],"title":"My Cloud Homelab Setup"},{"categories":null,"contents":"I love using Projects to organise my chats. Aside from the organisational aspects of Projects, it is such a powerful way to interact with ChatGPT because it allows you to add custom instructions and files specifically related to the project. This is very important as it provides context for the model, and context is key when interacting with Large Language Models.\nLast weekend, I sat here trying to create the perfect custom project instructions and, despite my best efforts, I was not entirely satisfied with the output. I realised I was spending way too much time manually drafting custom instructions for my ChatGPT Projects. I kept running into the same problem: prompt structure. Then the penny dropped: I realised I could just use ChatGPT to help me build this, and so I built a Prompt Generator GPT. I worked iteratively with GPT-5, asking the model to ask me clarifying questions to help me design a prompt which will allow me to build project instructions for a wide range of subjects.\nHere’s how it works, why I built it, and of course, I will share the final prompt. Lets get into it.\nThe Prompt Framework As mentioned previously, context is key to building effective prompts. But creating prompts is not easy. There is a craft to it, and having watched and read a lot of tutorials about designing effective prompts, I settled on the following principles:\nRole: Defines the role/expertise the model should adopt. Goal: States the main outcome I want the model to deliver. Context: Provides background, audience, or purpose to frame the model’s response. Instructions: Outlines step-by-step guidance on how the model should approach the task. Examples: Supplies sample outputs or demonstrations to shape the model’s style and approach. Format: Specifies the structure for the response, such as lists, steps, tables, or narrative. Constraints: Lists the rules, limitations, or must-have conditions for the output. Tone: Defines the style or voice the response should use, e.g. formal, conversational, playful. I should state that these are just principles, a prompting compass if you like. I do not use all of the principles in every prompt—it really depends on what outcome I am trying to achieve. I have got into the habit of keeping these in mind when I am prompting, and it has definitely helped me become more thoughtful about what I want to achieve and has helped me build better prompts. With that said, I am human, and I often do not get the prompt right first time around. This is where iteration is important: you have to keep working with the model until you get what you need.\nThe Idea: A Prompt Generator Using the principles which I mentioned previously and through a lot of practice and experimentation, I am fairly comfortable building prompts, but I am certainly not an expert. I also find it exhausting sometimes trying to build the perfect prompt, and I have lost many hours to the process. This is where the idea of building a Prompt Generator arrived. My objectives were clear:\nCreate a custom GPT designed to ask me smart clarifying questions about what I’m trying to achieve. Use my answers to generate a structured XML-style prompt template. Use the prompt template as custom instructions for my projects. This way, I could ensure that my project instructions have a repeatable structure, regardless of the subject. Instead of reinventing the wheel each time, I’d have a reliable structure to work from. This consistency means that when I return to a project weeks later, I do not have to re-learn how I structured the instructions or worry about missing an important detail. It also frees me up creatively, since the framework handles the structure and I can focus on the actual content. In short, the Prompt Generator gives me a foundation that is predictable, reusable, and adaptable, no matter what topic I am working on.\nThe Design Process By working with GPT-5 in an iterative manner, here is the step-by-step logic that we built into the Prompt Generator GPT:\nConfirm the subject that I want to work on. Ask smart questions to refine the purpose, goals, depth, tone, and any constraints. Present a set of preset teaching/explaining styles (like Feynman or Socratic) and help me pick one. Present a set of output formats (lists, steps, tables, etc.). Generate relevant examples if I have not supplied any. Apply base constraints: always factual, concise, British English. Build the XML prompt, only including tags that are needed. Show me the draft and ask if I want to tweak it further. The Final Prompt Here’s the final prompt I settled on. Since implementing this, I have generated some really useful custom instructions for my projects. Each project has a custom prompt which provides the necessary context and directs the model to produce the desired output. The cool thing about this Prompt Generator is that it is adaptable and can be modified in the future should I wish to enhance its capabilities. For now, though, I am happy with the structure, and it fulfils my goal of generating structured custom project instructions and also frees me up to create and build more.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 \u0026lt;role\u0026gt; You are a world-class Prompt Engineer. Your purpose is to help me design tailored prompts for any subject. You must always begin by asking clarifying questions about my subject, goals, preferred output, and any special requirements. Use these answers to generate a clean XML-based prompt framework, omitting unused tags. \u0026lt;/role\u0026gt; \u0026lt;process\u0026gt; 1. Greet me and confirm the subject I want to design a prompt for. 2. Ask clarifying questions to refine purpose, goals, depth, tone, and any constraints. 3. Present the preset teaching/explaining styles below, suggest the most suitable one, and ask me to confirm or override. 4. Present the default output format options below. Suggest the most suitable one, and ask me to confirm or override. 5. Generate relevant examples if none are provided. 6. Always include base constraints: factually accurate, concise, British English. Add subject-specific constraints if needed. 7. Build the framework with XML tags, omitting unused ones. 8. Present the draft. Ask if I’d like to iterate further. \u0026lt;/process\u0026gt; \u0026lt;preset_styles\u0026gt; \u0026lt;Socratic\u0026gt;Guided discovery through questioning.\u0026lt;/Socratic\u0026gt; \u0026lt;Feynman\u0026gt;Plain-language explanations.\u0026lt;/Feynman\u0026gt; \u0026lt;Tutor\u0026gt;Step-by-step with practice.\u0026lt;/Tutor\u0026gt; \u0026lt;Storyteller\u0026gt;Narrative-driven learning.\u0026lt;/Storyteller\u0026gt; \u0026lt;Analyst\u0026gt;Structured breakdowns.\u0026lt;/Analyst\u0026gt; \u0026lt;/preset_styles\u0026gt; \u0026lt;default_output_formats\u0026gt; - Bulleted lists - Numbered steps - Tables - Narrative paragraphs - Q\u0026amp;A style - Mixed format \u0026lt;/default_output_formats\u0026gt; \u0026lt;framework_structure\u0026gt; \u0026lt;role\u0026gt; [Defines the role/expertise the model should adopt] \u0026lt;/role\u0026gt; \u0026lt;goal\u0026gt; [What I want to achieve] \u0026lt;/goal\u0026gt; \u0026lt;context\u0026gt; [Relevant background, audience, or purpose] \u0026lt;/context\u0026gt; \u0026lt;instructions\u0026gt; [Step-by-step guidance for how the model should act] \u0026lt;/instructions\u0026gt; \u0026lt;examples\u0026gt; [Up to 3 examples generated or provided] \u0026lt;/examples\u0026gt; \u0026lt;format\u0026gt; [Preferred structure] \u0026lt;/format\u0026gt; \u0026lt;constraints\u0026gt; [Defaults + subject-specific] \u0026lt;/constraints\u0026gt; \u0026lt;tone\u0026gt; [Style or tone of output] \u0026lt;/tone\u0026gt; \u0026lt;/framework_structure\u0026gt; A Real World Example In this example I used the prompt generator to design a customised prompt for learning about Kubernetes as a beginner.\nHere is my initial prompt. The prompt is basic by design, it provides a general idea, which is fine because the Prompt Generator will ask clarifying questions to help build the final prompt:\nHere are the clarifying questions from the model:\nHere are my responses to the clarifying questions:\nHere is the final prompt:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 \u0026lt;framework\u0026gt; \u0026lt;role\u0026gt; You are a Kubernetes instructor and cloud-native mentor with expertise in Docker, Kubernetes, and AKS. \u0026lt;/role\u0026gt; \u0026lt;goal\u0026gt; Help the learner build practical Kubernetes skills for deploying and managing applications in AKS, starting from beginner-level basics and gradually advancing toward production-grade setups. \u0026lt;/goal\u0026gt; \u0026lt;context\u0026gt; The learner is familiar with containers through Docker but is new to Kubernetes. They will be using Kubernetes professionally to move applications into AKS. Learning should begin with gentle, practical introductions and progress steadily to advanced topics. \u0026lt;/context\u0026gt; \u0026lt;instructions\u0026gt; 1. Begin each lesson with a practical task (e.g., deploying a pod, creating a service). 2. Explain the underlying concept only after showing the task in action. 3. Provide examples using `kubectl` commands and YAML manifests. 4. Connect each step to real-world AKS usage (authentication, scaling, monitoring). 5. Use a combination of methods: - \u0026lt;Feynman Technique\u0026gt;: keep explanations simple and plain. - \u0026lt;Problem-Based Learning\u0026gt;: introduce small deployment challenges to solve. - \u0026lt;Elaboration\u0026gt;: add “why” and “how” explanations for decisions. - \u0026lt;Dual Coding\u0026gt;: include diagrams or tables when helpful (e.g., Pods vs Deployments). 6. Present material in a mixed format (steps, tables, narrative, Q\u0026amp;A). 7. Keep explanations concise, accurate, and in British English. 8. Only generate flashcard-style quizzes if explicitly requested by the learner. \u0026lt;/instructions\u0026gt; \u0026lt;examples\u0026gt; \u0026lt;example1\u0026gt; Task: Deploy a simple Nginx pod in AKS. - Show `kubectl run nginx --image=nginx --restart=Never` - Explain what a Pod is, why it’s the smallest unit in Kubernetes. - Show how to check status with `kubectl get pods`. \u0026lt;/example1\u0026gt; \u0026lt;example2\u0026gt; Task: Expose the pod with a service. - Show `kubectl expose pod nginx --port=80 --type=ClusterIP` - Explain service types (ClusterIP vs LoadBalancer vs NodePort). - Connect to real-world: “In AKS, you typically use LoadBalancer to expose apps externally.” \u0026lt;/example2\u0026gt; \u0026lt;example3\u0026gt; Task: Scale the application. - Show `kubectl scale deployment nginx-deployment --replicas=3` - Explain why deployments are used for scaling and self-healing. \u0026lt;/example3\u0026gt; \u0026lt;/examples\u0026gt; \u0026lt;format\u0026gt; Mixed format (commands, steps, tables, short explanations, and occasional Q\u0026amp;A). \u0026lt;/format\u0026gt; \u0026lt;constraints\u0026gt; - Ensure all explanations are factually accurate. - Keep language clear, concise, and in British English. - Always connect Kubernetes tasks back to real-world AKS usage. - Start with beginner-friendly tasks, then gradually introduce advanced concepts. \u0026lt;/constraints\u0026gt; \u0026lt;tone\u0026gt; Friendly, practical, and professional — like a mentor guiding hands-on learning. \u0026lt;/tone\u0026gt; \u0026lt;quiz_mode\u0026gt; Disabled by default (only generate flashcards when explicitly requested). \u0026lt;/quiz_mode\u0026gt; \u0026lt;/framework\u0026gt; Final Thoughts If you’re struggling with the process of designing effective prompts or, like me, just spending too much time trying to perfect the craft, then I highly recommend setting up something like this. It’s like having an expert prompt engineer on hand who can design the prompts you are trying to build in a way that produces better output.\nThis has already saved me hours and made my outputs more consistent and targeted. If you want to try it yourself, feel free to copy my setup, tweak it, and make it your own. I’d also love to hear your take - if you have tricks or tools for building better prompts, share them back.\nLet me know if you build your own version - I’d love to see how others are solving prompt fatigue.\n","permalink":"https://blog.autonate.dev/post/building-my-own-prompt-generator-gpt/","tags":["ai","llm","chatgpt","prompt engineering"],"title":"Building My Own Prompt Generator GPT"},{"categories":null,"contents":"If there’s one AI tool I reach for every day without fail, it’s Perplexity. It has become my go-to assistant for staying up to date with the latest news and exploring new topics. I wanted to share how I use it, why I keep coming back to it, and a few features I think are genuinely worth exploring.\nMy Two Main Use Cases: News and Learning I use Perplexity primarily in two ways:\nTo stay informed – I check in with it throughout the day to catch up on current events. In the fast-paced world we live in, it has been such a useful tool to help me stay updated on current affairs. It is accurate, focused, and helps me cut through the noise — and we all know there is plenty of noise.\nTo learn new skills – Whether it’s a concept I’ve come across in tech, a historical event I want to understand better, or a tool I want to explore, Perplexity serves as a solid research assistant and resident expert on the topic at hand.\nThe way Perplexity distils information and key concepts definitely helps me retain information more efficiently and makes me feel like I’m getting smarter. Whether I am or not is still up for debate.\nHey Human! Source Checking Still Matters Caveat: Please, do not forget that AI tools should facilitate human thinking — don’t neglect the magnificence of the human brain amidst the hype! I always make it a priority to check the original sources that these tools reference, and I encourage you to do the same. Even though I usually find Perplexity to be accurate, AI hallucinations still happen — and it’s on us as users to verify what we’re reading.\nI’ve tested Perplexity a fair bit by reading full articles first, then prompting it to summarise them. In most cases, the summary reflects the content well — which speaks to the quality of its comprehension. However, it’s not always accurate, so you should always double-check.\nThe iOS App: A Handy Everyday Companion I use an iPhone, and the iOS app is really well made. I keep the Perplexity widget on my Lock Screen for fast access, which is perfect for when something crosses my mind and I want a quick answer.\nVoice mode is included too. It’s not quite at ChatGPT’s level in terms of how natural the voice assistant sounds, but it’s still solid and fast enough for casual use.\nThe Discover Tab: Curated Curiosity Another feature I use quite a lot is Discover. This section highlights trending or interesting topics being searched and explored by others. I’ve stumbled across some genuinely useful and insightful stories here. It has also helped me discover new blogs and websites — and that’s a very nice side effect!\nThat said, I do find it to be a bit too US-centric. I’d love to see Perplexity introduce more regional curation in the future. For example, I’m in the UK and have no interest in American sports like NFL and MLB — but despite my best efforts at disliking these stories, they still seem to sneak in. For the most part, though, it’s a great way to stay informed and see what’s trending.\nGrouping Ideas with Spaces Perplexity Spaces is a standout feature for anyone working across multiple projects or areas of interest. If you’re not using it yet, here’s why you might want to:\nSpaces let you group related chats together — think of it like folders for your AI conversations.\nYou can set custom instructions (essentially a system prompt), which apply to every conversation within that space.\nFor example, I have Spaces dedicated to:\nAI Tools Current Affairs 20th Century History 90s Culture Each Space carries its own context, so I don’t need to repeat myself when returning to a topic. It’s especially useful if you’re doing ongoing research or want more structured results tailored to a specific theme.\nPlaying with New Models One of the big benefits of using Perplexity is how fast they roll out access to new AI models. You can switch between them easily and compare how they handle different types of queries. Some are better for reasoning, others for speed or summarisation — and it’s fun to test and learn the differences to understand which model works best.\nDon’t Sleep on “Labs” A recent feature that I think deserves more attention is Perplexity Labs. In their blog post introducing Labs, Perplexity says that “Using Labs is like having a team.” Think of Labs as a lightweight way to prototype useful tools. It enables users to transform ideas and research into fully realised outputs — such as reports, spreadsheets, dashboards, and even basic web apps — in minutes.\nActing like a virtual project team, Labs combines advanced AI capabilities including deep research, code execution, and visualisation tools to handle complex, multi-step workflows that would otherwise take days and require multiple skill sets. Everything generated is organised in dedicated tabs for easy access, download, and sharing — making it ideal for business analysis, marketing plans, or personal projects. Accessible via the mode selector on web and mobile, Perplexity Labs empowers anyone to turn curiosity into action without the need for coding or juggling multiple apps — streamlining the path from idea to execution.\nFinal Thoughts Perplexity continues to impress me. It’s fast, reliable, and constantly evolving. Whether you’re chasing a breaking news story, exploring a new topic, or experimenting with the latest models, it’s a powerful companion for curiosity and learning.\nIt’s rare to find a product that balances usability with cutting-edge features so well. I’m genuinely a happy customer, and if you haven’t tried it yet, I highly recommend giving it a go.\n","permalink":"https://blog.autonate.dev/post/how-i-use-perplexity-to-stay-informed-and-learn-new-skills/","tags":["ai","llm","perplexity"],"title":"How I Use Perplexity to Stay Informed and Learn New Skills"},{"categories":null,"contents":"In my previous post about Tailscale, I covered what Tailscale is and why it’s become a staple in my homelab toolkit. Today, I want to share a real-world example of just how handy it can be—specifically, how it helped me retrieve personal documents from my homelab while sat on a train, armed with nothing more than my iPhone and a 5G connection.\nThe Situation: Needing Remote Access on the Go We’ve all been there: you need a file, but it’s tucked away on your home network, and you’re nowhere near your desk. For me, this happened while I was on a train, miles away from home. The documents I needed were only accessible from my local network—or so I thought.\nConnecting via Tailscale and Termius Thanks to Tailscale, my home devices are part of a secure mesh network, no matter where I am. Using my iPhone, I connected to my Tailscale network over 5G. From there, I fired up the Termius app, which provides a straightforward SSH client for iOS.\nWith Termius, I initiated an SSH session directly to my homelab server—the same machine quietly humming away under my desk. Within seconds, I was browsing my files as if I were sitting at home.\n1 2 # Example SSH command in Termius ssh username@homelab-server From there, it was a simple matter of copying the documents I needed and sending them to myself securely.\nWhy Tailscale Makes This Possible Tailscale’s magic lies in its ability to make remote devices feel local, securely and effortlessly. There was no need to mess around with port forwarding, set up a VPN server, or worry about exposing services to the wider internet. Everything just worked, seamlessly and securely.\nFinal Thoughts This experience was a timely reminder of why I’m glad to be using Tailscale. It transforms the way I think about remote access—making my homelab available wherever I am, with minimal fuss. If you’re managing a homelab or need secure access to your home network on the go, I can’t recommend Tailscale enough.\n","permalink":"https://blog.autonate.dev/post/tailscale-saves-the-day/","tags":["tailscale","homelab"],"title":"Tailscale Saves the Day"},{"categories":null,"contents":"Managing configuration files across multiple systems can quickly become a mess. From terminal preferences to shell settings, we all have a finely-tuned setup that we rely on day-to-day. But how do you keep those settings consistent across machines? And how do you avoid the chaos of losing them after a reinstall?\nThat’s where a tool like Chezmoi comes in.\nWhat is Chezmoi? Chezmoi is a powerful dotfile manager designed to track, version, and securely deploy your configuration files across multiple operating systems. It helps you manage configuration files such as ~/.zshrc, ~/.gitconfig, ~/.config/ and more — all with a simple Git workflow.\nUnlike traditional symlink managers like GNU Stow, Chezmoi works by maintaining a source state in a Git repository, which it applies to your home directory. This means you can track your configuration files, safely test changes, and even add logic for different operating systems or hosts.\nGetting Started To get started with Chezmoi, install it using your preferred package manager.\nOn macOS:\n1 brew install chezmoi Ubuntu:\n1 sudo apt install chezmoi Windows:\n1 winget install twpayne.chezmoi Then initialise your dotfiles repository:\n1 chezmoi init --apply \u0026lt;repo\u0026gt; This command clones your dotfiles repository (e.g. https://github.com/aut0nate/dotfiles) and immediately applies the configuration to your home directory. You can also start fresh by running:\n1 chezmoi init And then begin adding files:\n1 2 chezmoi add ~/.zshrc chezmoi add ~/.tmux.conf The Power of Templates One of Chezmoi’s most useful features is its powerful templating engine. You can create conditional logic in your config files using Go templating syntax.\nFor example, let’s say you want a slightly different .zshrc on macOS vs Linux:\n1 2 3 4 5 {if eq .os \u0026#34;darwin\u0026#34;} echo \u0026#34;Running on macOS\u0026#34; {else if eq .os \u0026#34;linux\u0026#34;} echo \u0026#34;Running on Linux\u0026#34; {end} You can also target OS-specific files by using suffixes in your source directory:\n1 2 ~/.local/share/chezmoi/dot_zshrc.tmpl ~/.local/share/chezmoi/dot_gitconfig.tmpl Or host-specific files:\n1 2 3 4 5 dot_gitconfig.tmpl └── applies to all systems dot_gitconfig@workmachine.tmpl └── only applies when the hostname is \u0026#39;workmachine\u0026#39; This allows you to have one unified repository that works intelligently across every system you manage. Click the link to learn more about templates in Chezmoi.\nWhy Backing Up Config Files Matters Your configuration files represent hours — if not years — of customisation. Losing them can feel like losing a part of your workflow.\nBy using Chezmoi with Git, you:\nTrack every change you make to your configs Back up your entire setup to GitHub or another Git provider Easily restore your environment after a system wipe or when setting up a new machine Reduce time spent reconfiguring new environments Your dotfiles essentially become portable, versioned, and secure — which is especially useful if you work across macOS, Linux, or WSL environments like I do.\nMy Dotfiles If you\u0026rsquo;re curious, you can check out my dotfiles on GitHub. Feel free to use it for inspiration in your own configuration file management system.\nFinal Thoughts Chezmoi has quickly become one of my essential tools. It removes the friction of managing configuration files and I can rest easy knowing that everything is under source control and available to me whenever I need it. For example I recently received a new laptop at work, and with a simple chezmoi init my configuration files were available to me, without the hassle of setting everything up from scratch.\nIf you are looking for a tool to help you manage your configuration files across multiple platforms, Chezmoi is a great place to begin.\n","permalink":"https://blog.autonate.dev/post/managing-dotfiles-the-smart-way-with-chezmoi/","tags":["cli","devops","cli"],"title":"Managing Dotfiles the Smart Way with Chezmoi"},{"categories":null,"contents":"Tailscale has transformed the way I manage and access my homelab, providing seamless, secure connectivity between all my devices—wherever I am. In my previous post, \u0026ldquo;How Tailscale Transformed Secure Access to My Homelab\u0026rdquo;, I shared how Tailscale has become an essential part of my daily workflow, enabling remote access to servers, VMs, and services without exposing anything to the public internet. Today, I want to highlight another standout feature: Taildrop.\nWhat is Taildrop? Taildrop is a built-in feature of Tailscale that allows you to securely send files between any devices connected to your Tailnet. Taildrop transfers files over encrypted peer-to-peer connections, using the fastest available path. This makes it a great solution for sending sensitive or large files without third-party servers in the middle.\nTaildrop works by leveraging the encrypted tunnels that Tailscale creates between your devices, ensuring that your files are never exposed to the public internet. Whether you’re sending photos from your phone to your desktop, or sharing log files between devices, Taildrop makes it as easy and secure.\nWhy Use Taildrop? Security: All file transfers are encrypted and happen directly between your devices over Tailscale’s secure network. Simplicity: No need to set up additional software or services. Taildrop is built right into Tailscale. Flexibility: Works across all major platforms—Windows, macOS, Linux, iOS, and Android. Speed: File transfers are sent peer-to-peer using the fastest route, maximising transfer speeds. How I Use Tailscale (and Taildrop) Every Day Tailscale is at the heart of my homelab and I use it to:\nRemotely access my homelab servers and VMs from anywhere, as if I’m on the same local network. Securely share files between devices using Taildrop—whether it’s a photo or a log file. Taildrop, in particular, has saved me countless hours. Instead of using scripts to transfer files between devices, I simply use Taildrop to transfer files quickly and securely. It’s especially handy when I’m troubleshooting issues on remote machines and I want to ensure that files are available on other devices within my Tailnet.\nHow Taildrop Works To use Taildrop, you simply need to have Tailscale installed on both devices. From there, file sharing is straightforward:\nOn desktop platforms, you can use the built-in sharing capabilities within the OS (usually right clicking a file) to share any file with Taildrop.\nOn mobile devices, you can use the built-in iOS or Android sharing capabilities to share any file with Taildrop.\nFor detailed instructions and examples, check out the official Taildrop documentation.\nExample: Sending a File with Taildrop Here’s a quick example of how you might send a file between devices in your tailnet. In this example, I sent a pdf from macOS to Windows 11:\nI located the pdf which I wanted to share and right clicked on the file. From the list of options I click Share and selected Tailscale:\nI was presented with a list of devices in my Tailnet, I cicked Send on my arkham-win11 device:\nJust like that, the pdf was available to me on my Windows 11 device:\nFinal Thoughts Taildrop is a powerful, yet simple tool that exemplifies the best of what Tailscale offers: secure, private, and effortless connectivity between all your devices. As someone who uses Tailscale every day, I can’t recommend Taildrop enough for anyone who needs to share files securely—whether you’re managing a homelab, collaborating with a team, or just moving files between your own devices.\nIf you haven’t tried Tailscale or Taildrop yet, I highly encourage you to give them a go. For more about how I use Tailscale in my homelab, check out my previous post: How Tailscale Transformed Secure Access to My Homelab.\n","permalink":"https://blog.autonate.dev/post/taildrop-secure-file-sharing-from-anywhere-with-tailscale/","tags":["tailscale","homelab","networking"],"title":"Taildrop: Secure File Sharing from Anywhere with Tailscale"},{"categories":null,"contents":"Connecting to services in a homelab can be a headache, especially when you want strong security without the hassle of port forwarding, firewall rules, or wrestling with certificates. Last year, I discovered Tailscale and introduced it to my homelab. Tailscale has become my solution for secure, reliable, and zero-fuss access to certain services in my homelab—which I do not want to expose via the public internet. In this post, I’ll walk through what makes Tailscale so effective, how to set it up, and the best practices I’ve picked up along the way.\nWhat is Tailscale? Tailscale is a zero-config VPN built on WireGuard, designed to make your devices communicate as if they’re on the same local network—no matter where they are. Each device runs a lightweight agent that forms encrypted, peer-to-peer tunnels to other devices in your network (your “tailnet”). A central coordination server handles authentication and key distribution, but never sees your actual traffic. Access control is managed centrally with simple ACL rules, making it easy to define exactly who or what can access your services.\nWhy Tailscale for the Homelab? Before Tailscale, remote access to my homelab meant juggling dynamic DNS, opening ports, and watching firewall logs for suspicious activity. With Tailscale, every device—laptop, phone, server, or VM—gets a stable IP and hostname, and all traffic is end-to-end encrypted. I can SSH into my servers, access web dashboards, or even stream media from anywhere, without exposing a single port to the public internet.\nKey benefits for homelabbers:\nZero-trust by default: No device can talk to another unless explicitly allowed. No port forwarding: Everything works behind NAT. Granular access control: ACLs let you tightly control who can access what. Works everywhere: macOS, Windows, Linux, iOS, Android, and even Docker containers. Getting Started: Setting Up Tailscale 1. Create a Tailscale Account Signing up is as simple as authenticating with Google, Microsoft, GitHub, or Apple. Just head to tailscale.com, click Get Started, and log in with your preferred identity provider. You’ll land in the admin console, ready to start building your tailnet.\n2. Install Tailscale on Your Devices macOS Download the installer from tailscale.com/download or the App Store. Run the .pkg file and follow the prompts. Open Tailscale, sign in, and allow VPN configuration. Windows Download the .msi installer from tailscale.com/download. Run it, open Tailscale from the Start menu, and log in. Linux 1 2 curl -fsSL https://tailscale.com/install.sh | sh sudo tailscale up If your distro uses systemd, enable autostart:\n1 sudo systemctl enable --now tailscaled iOS Install Tailscale from the App Store. Open the app, log in, and allow VPN configuration. 3. Confirm Devices in Your Tailnet Head to login.tailscale.com/admin/machines and make sure all your devices appear as Connected. Each one gets a unique Tailscale IP and hostname.\n4. Test Connectivity From one device, try pinging another using its Tailscale IP or hostname:\n1 2 ping 100.x.x.x ssh \u0026lt;user\u0026gt;@\u0026lt;hostname\u0026gt; On Windows, use Command Prompt:\n1 ping 100.x.x.x On iOS, an app like Termius lets you SSH into other devices.\nFine-Tuning Access: Using ACLs Tailscale’s Access Control Lists (ACLs) are a powerful way to define who can access what in your tailnet. The ACLs are managed centrally in a JSON file via the admin console (login.tailscale.com/admin/acls).\nExample ACL:\nAllow your MacBook Pro and iPhone to access everything. Prevent lab/dev machines from accessing your personal devices. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \u0026#34;hosts\u0026#34;: { \u0026#34;macbook-pro\u0026#34;: \u0026#34;ip-1\u0026#34;, \u0026#34;iphone\u0026#34;: \u0026#34;ip-2\u0026#34;, \u0026#34;arkham-lab\u0026#34;: \u0026#34;ip-3\u0026#34;, \u0026#34;arkham-dev\u0026#34;: \u0026#34;ip-4\u0026#34;, \u0026#34;arkham-app\u0026#34;: \u0026#34;ip-5\u0026#34;, \u0026#34;arkham-dev-pc\u0026#34;: \u0026#34;ip-6\u0026#34; }, \u0026#34;acls\u0026#34;: [ { \u0026#34;action\u0026#34;: \u0026#34;accept\u0026#34;, \u0026#34;src\u0026#34;: [ \u0026#34;macbook-pro\u0026#34;, \u0026#34;iphone\u0026#34; ], \u0026#34;dst\u0026#34;: [\u0026#34;*:*\u0026#34;] }, { \u0026#34;action\u0026#34;: \u0026#34;accept\u0026#34;, \u0026#34;src\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;dst\u0026#34;: [ \u0026#34;arkham-lab:*\u0026#34;, \u0026#34;arkham-app:*\u0026#34;, \u0026#34;arkham-dev:*\u0026#34;, \u0026#34;arkham-dev-pc:*\u0026#34; ] } ] } By default, devices can’t talk to each other unless you add rules. This “least privilege” approach keeps your network tight and secure.\nTesting ACLs You can add test rules to verify your ACLs work as intended. For example, check if arkham-app can access SSH on your MacBook Pro (it shouldn’t):\n1 2 3 4 5 6 7 8 9 { \u0026#34;tests\u0026#34;: [ { \u0026#34;src\u0026#34;: \u0026#34;arkham-app\u0026#34;, \u0026#34;proto\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;accept\u0026#34;: [\u0026#34;macbook-pro:22\u0026#34;] } ] } If the test result is “Drop”, your ACL is working.\nEnhancing Usability: MagicDNS and HTTPS MagicDNS Enable MagicDNS in the admin console (login.tailscale.com/admin/dns) to resolve device hostnames instead of memorising IPs. Now you can SSH into arkham-lab.tailnet-name.ts.net instead of 100.x.x.x.\nHTTPS Certificates You can also enable HTTPS certificates for your devices, making it easy to securely access web services with trusted SSL—no manual certificate wrangling required.\nConfiguring Global Nameservers Tailscale lets you specify global DNS servers for your tailnet. In the DNS settings, add your preferred nameserver (e.g., Cloudflare, Google) and enable the Override DNS servers toggle. This ensures all devices use your chosen DNS, improving consistency and privacy.\nTroubleshooting Tips If a device doesn’t appear in your tailnet, check that the Tailscale agent is running and logged in. For connectivity hiccups, try restarting the Tailscale service or re-authenticating. Use the admin console’s ACL tests to debug access issues before they become headaches. Final Thoughts Tailscale has dramatically simplified secure access to my homelab. With its zero-config setup, robust access controls, and seamless cross-platform support, I can connect to any service, anywhere, without exposing my network to unnecessary risk. Whether you’re running a single Raspberry Pi or a rack of servers, Tailscale makes secure networking effortless—and once you’ve tried it, you’ll wonder how you ever managed without it.\n","permalink":"https://blog.autonate.dev/post/how-tailscale-transformed-secure-access-to-my-homelab/","tags":["tailscale","homelab","networking"],"title":"How Tailscale Transformed Secure Access to My Homelab"},{"categories":null,"contents":"In my work life, I use a company-issued Windows 11 laptop. I genuinely enjoy working across all the major operating systems, from Linux and macOS to Windows itself—I’ve found that Windows has evolved in some really interesting ways, especially when it comes to Linux and Open Source software.\nOne of the very first things I do when setting up a new Windows system is make sure the Windows Subsystem for Linux (WSL2) is installed and configured just the way I like it. I’ve been using WSL on a daily basis for years, and it’s become an essential part of my workflow, allowing me to run all the Linux tools I need in a familiar, integrated environment. Plus, it’s improved massively since I first tried it out on Windows 10 back in 2017.\nIn this post, I’ll explain why WSL matters, how Microsoft’s approach to open source has shifted, and share some of my favourite power user tips to help you get the most out of it.\nWhat is WSL and Why Should You Care? WSL is Microsoft’s way of bringing the Linux experience to Windows—without the fuss of dual booting or running a heavyweight virtual machine. It lets you run a full Linux environment right inside your Windows setup, complete with access to all your favourite command-line tools, package managers, and even GUI apps thanks to WSLg.\nFor me, this means I can write and test Bash scripts, manage Linux servers, and run open source tools—all from the comfort of my Windows desktop. It’s perfect for developers, sysadmins, and anyone who wants to bridge the gap between Windows and Linux.\nWhy is Microsoft Suddenly All In on Open Source? If you’ve been around the tech world for a while, you’ll remember when Microsoft and open source were sworn enemies. But times have changed. Under Satya Nadella, Microsoft has embraced open source in a big way—contributing to the Linux kernel, open sourcing .NET, and even buying GitHub.\nMicrosoft’s commitment to WSL didn’t stop at just improving its features and performance—they went one step further. At the Build 2025 Conference, Microsoft announced that WSL2 is now open source (most of), marking a significant milestone for developers everywhere. This means anyone can download, inspect, modify, and contribute to the WSL codebase on GitHub, fostering greater transparency and community collaboration. It’s a clear signal that Microsoft is serious about supporting open source and empowering developers to shape the future of Linux on Windows\nSo why the shift? It’s all about staying relevant. The cloud, DevOps, and modern software development are built on open source. By supporting tools like WSL, Microsoft is making Windows more attractive to developers and IT pros who need to work in both worlds.\nWhy Should You Use WSL? Here are a few reasons why I appreciate WSL:\nAccess to Linux Tools: Run your favourite cli tools and package managers right from Windows. Seamless Workflows: No more switching between operating systems or juggling VMs. Cross-Platform Development: Test your code on Linux and Windows from the same machine. Lightweight and Fast: WSL starts up quickly and uses fewer resources than a full VM. Integration: Use Linux GUI apps on your Windows desktop and access files between both systems with ease. How to Install and Enable WSL on Windows 11 Microsoft has made installing WSL incredibly straightforward. Here’s how you can do it, step by step:\nOpen Windows Terminal (Admin):\nRight-click the Start button and select Terminal (Admin) or Windows Terminal (Admin). If prompted by User Account Control (UAC), click Yes to allow changes.\nInstall WSL:\nIn the Terminal window, enter the following command:\n1 wsl --install This command will:\nEnable the required Windows features (including WSL and the Virtual Machine Platform). Download and install the latest Linux kernel. Install Ubuntu as the default Linux distribution (unless you specify otherwise). Restart Your Computer:\nAfter the installation completes, you’ll be prompted to restart your computer. This is necessary for the changes to take effect:\n1 Restart-Computer Or simply use the Windows Start menu to restart.\nSet Up Your Linux Distribution:\nAfter restarting, open Windows Terminal again (you don’t need admin this time). Ubuntu (or your chosen distro) will start automatically.\nYou’ll be asked to create a new UNIX username and password (this is for Linux, not your Windows account).\nOnce set up, you can update your Linux distribution by running:\n1 sudo apt update \u0026amp;\u0026amp; sudo apt upgrade This ensures you have the latest packages and security updates.\n(Optional) Install a Different Linux Distribution:\nIf you want to install a distro other than Ubuntu you can list the available distributions:\n1 wsl --list --online Install your preferred distro (replace DistroName with the name from the list):\n1 wsl --install -d DistroName For example, to install Debian:\n1 wsl --install -d Debian Launch and Use WSL:\nYou can launch your Linux distribution from the Start menu or directly from Windows Terminal. Access Linux commands and tools as you would on a native Linux system.\nWSL Configuration Settings I recommend that you customise WSL to your requirements by making use of the .wslconfig and wsl.conf files. There are a wide range of configuration settings available and it is worth checking them out by visiting Advanced settings configuration in WSL.\n1. Customise .wslconfig The .wslconfig is located at C:\\Users\\\u0026lt;UserName\u0026gt; and allows you to configure global settings across all installed distributions running on WSL.\nExample .wslconfig: 1 2 3 4 5 6 7 [wsl2] memory=4GB processors=2 swap=4GB localhostForwarding=true nestedVirtualization=false vmIdleTimeout=60 2. Customise wsl.conf The wsl.conf is located within your Linux distro at /etc/wsl.conf and allows you to configure settings per-distribution for WSL.\nExample wsl.conf: 1 2 3 4 5 6 7 8 9 10 11 [boot] systemd=true [network] generateResolvConf = true [user] default=\u0026lt;user\u0026gt; [interop] enabled = true Interoperability Between Windows \u0026amp; Linux WSL makes it easy to mix Windows and Linux tools:\nOpen Windows Explorer from WSL:\n1 explorer.exe . Run Windows apps from WSL:\n1 notepad.exe .bashrc Access WSL files from Windows: Enter \\\\wsl$\\ in Windows Explorer to browse your Linux files.\nRun Linux Apps on Windows WSL allows you to run Linux GUI based apps on Windows. For example, you can run the Nautilus graphical file manager in WSL on Windows:\nInstall Nautilus:\n1 2 sudo apt update sudo apt install nautilus -y Launch Nautilus:\n1 sudo nautilus (Requires WSLg or an X server for GUI support.)\nSupercharge Your Shell In Linux there are many shells to choose from such as bash, zsh and fish. I recommend exploring different shells and finding which works best for you. I prefer to use zsh with ohmyzsh. If you want to follow a similar configuration, I recommend this short and easy guide to get it configured Oh My Zsh + PowerLevel10k = 😎 terminal.\nHere is how my WSL shell currently looks:\nI keep all my configuration files—such as .zshrc, .tmux.conf, and shell aliases—under source control. This approach means that no matter what system I’m using—whether it’s WSL on Windows, a macOS terminal, or a Linux desktop—I can quickly sync my settings and enjoy a consistent, familiar shell environment everywhere. It’s a simple trick that saves time and ensures I feel right at home, no matter which platform I’m working on.\nFinal Thoughts WSL genuinely simplifies life for anyone who works across both Windows and Linux environments. Whether you’re streamlining your automation workflows, developing cross-platform software, or simply curious about new technology, WSL offers a seamless bridge between the two worlds. For developers, sysadmins, and tech enthusiasts alike, adding WSL to your toolkit means greater flexibility, productivity, and the freedom to work the way you want—making it an essential addition for those who want the best of both worlds.\n","permalink":"https://blog.autonate.dev/post/exploring-the-windows-subsystem-for-linux-wsl/","tags":["linux","windows","cli"],"title":"Exploring the Windows Subsystem for Linux (WSL)"},{"categories":null,"contents":" What is an API? APIs, short for Application Programming Interfaces, are tools that allow software applications to communicate with each other. They define a set of rules and protocols that enable developers to access features or data from another service without needing to understand or modify the underlying code. Think of them as messengers that deliver your request to another service and return the response back to you.\nThe Restaurant Analogy To better understand how APIs work, a popular and relatable analogy is that of a restaurant. Imagine you’re dining out:\nYou are the application (client) that wants something. The kitchen is the server or system that has what you need. The menu is the list of available options (API endpoints). The waiter is the API that takes your order (request), tells the kitchen what to do, and brings the food (response) back to your table. You don’t need to know how the kitchen operates behind the scenes or how each dish is made—you simply place your order, and the waiter ensures it gets to the kitchen and returns with your meal. Similarly, APIs allow you to interact with a service without needing to know the internal details of how it works.\nReal-World Examples of APIs in Action We interact with APIs all the time, often without realising it. Here are some everyday examples:\nWeather apps use APIs to fetch real-time weather data. Social logins (\u0026ldquo;Log in with Google/Facebook\u0026rdquo;) use APIs to authenticate users. Online payments rely on APIs like Stripe or PayPal to handle transactions. Maps and directions use APIs such as Google Maps to display routes and locations. Why APIs Matter to Developers For developers, APIs are powerful tools that save time and effort. Rather than building everything from scratch, they can:\nLeverage existing services and data Integrate new features quickly Focus on solving user-specific problems This modular approach accelerates development and improves the overall user experience.\nMy Use of the OpenRouter API I use a variety of APIs to interact with different services, and one I frequently rely on is the OpenRouter API. It acts as a gateway to a range of large language models (LLMs), allowing me to integrate powerful AI capabilities into my workflow with ease.\nHere are a few ways I use it:\nSummarising content: I can feed in long-form articles, technical documentation, or reports and get concise summaries that save me time. Extracting key insights: When working with PDFs or lengthy resources, the API helps me highlight essential points quickly. Getting answers: I can ask complex or niche questions and receive thoughtful, AI-generated responses. Using the OpenRouter API means I don\u0026rsquo;t have to build or host AI models myself. Instead, I send a simple request, and within seconds, I receive a detailed and relevant response. It empowers me to create intelligent tools and features with minimal setup and maximum flexibility.\nFinal Thoughts APIs are the backbone of modern applications. They streamline connectivity between tools and services, making our digital experiences seamless and efficient. Whether you\u0026rsquo;re a developer or just curious about how your apps function, understanding APIs helps you see the bigger picture.\nIf you\u0026rsquo;re new to the concept, I encourage you to explore APIs by building something simple—like pulling data from a public weather API or automating a task. There’s no better way to learn than by doing, and with so many APIs freely available, the possibilities are endless.\n","permalink":"https://blog.autonate.dev/post/understanding-apis-a-beginner-friendly-guide/","tags":["api"],"title":"Understanding APIs: A Beginner-Friendly Guide"},{"categories":null,"contents":"Back in 2020 I finally set aside time to learn about containerisation. Docker had been on my radar for a while, but I kept putting it off, convinced it was strictly developer territory. Determined to challenge that belief, I spun up a modest Virtual Private Server (VPS) with Contabo and embarked on a learn‑by‑doing journey, self‑hosting a handful of services. The concepts felt tricky at first, but I persevered, and as I grew comfortable with the Docker CLI, the hype began to make sense.\nWhat Is Docker? Docker is a virtualisation platform that allows you to package, distribute and run applications in lightweight, portable containers. Each container is an isolated environment holding everything the application needs—code, libraries and other dependencies. Docker eliminates the “it works on my machine” problem and ensures that software behaves consistently across environments.\nContainers vs Virtual Machines Containers Virtual Machines (VMs) Abstraction Level Operating‑system level; share the host kernel Hardware level; virtualise the entire machine Isolation Process isolation (namespaces \u0026amp; cgroups) Strong isolation with a full guest OS Resource Footprint Lightweight; start in seconds Heavier; require more CPU, RAM and storage Use‑Cases Cloud‑native, micro‑services, CI/CD pipelines Legacy workloads, monoliths, strong security boundaries Boot Time Near‑instant Tens of seconds to minutes The key takeaway is that containers virtualise the operating system, while VMs virtualise the hardware. That makes containers faster to start and more resource‑efficient, whereas VMs offer deeper isolation at the cost of additional overhead.\nBreaking the Myth – Docker Isn’t Just for Developers As I soon discovered, you don’t need deep programming knowledge to benefit from Docker. If you can follow a recipe and run a few terminal commands, you can run containers. Understanding how Docker works does deepen your appreciation, but you don’t have to be a developer to use it effectively. Once that realisation clicked, I started deploying more services and, as a side benefit, became far more comfortable at the command line. For those who prefer a GUI, Docker Desktop offers an excellent alternative.\nThe Learning Curve: YouTube, Blog Posts \u0026amp; KodeKloud My journey was fuelled by countless blog posts and YouTube tutorials. Concepts such as images, containers, volumes and networks felt abstract at first, but hands‑on practice soon turned those abstractions into muscle memory. As the saying goes, practice doesn’t make perfect—it makes permanent.\nI first discovered KodeKloud through their excellent two‑hour Docker crash course on YouTube. That single video became my launch‑pad and ultimately convinced me to subscribe to their Standard Plan. The quality of the lessons and interactive labs has further enhanced my knowledge.\nIf you’re just starting out, here are a few beginner‑friendly videos I still recommend:\nKodeKloud: Learn Docker in 2 hours\nTechWorld with Nana: Docker Crash Course for Absolute Beginners\nmCoding: Docker Tutorial for Beginners\nFireship: 100+ Docker Concepts you Need to Know\nDockerfiles, Images and Containers – A Quick Tour A Dockerfile is a plain‑text blueprint that lists every command required to assemble an image—think of it as a recipe. A typical Dockerfile begins with a base image and layers additional instructions on top:\n1 2 3 4 5 6 # Dockerfile example FROM ubuntu:18.04 # Base image RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y nginx # Install software COPY index.html /var/www/html # Copy project files CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] # Default command Key Components of a Dockerfile Base Image (FROM) – The starting point of your build, e.g. FROM node:20-alpine. Instructions – Commands such as RUN, COPY, ENTRYPOINT and CMD that install software, copy files or configure how the container starts. A Docker image is the packaged result of executing a Dockerfile. Images are:\nLayered – Each instruction in the Dockerfile adds a new immutable layer, cached for faster rebuilds. Immutable – To make changes you edit the Dockerfile and rebuild; the original image never mutates. Lightweight – Images do not contain a full OS, only the user‑space libraries needed for your application. A container is a running instance of that image—your application brought to life in an isolated environment.\nHow They Work Together Write a Dockerfile – Describe your environment, dependencies and runtime behaviour. Build an image – docker build -t my‑app . executes the Dockerfile and produces an image. Run a container – docker run -d --name my‑app my‑app starts the container from the image. The flow: Dockerfile → Image → Container.\nUseful Commands Here are some beginner‑friendly Docker commands that you’ll use often. To explore the full range, just run docker at the command line.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # Check Docker version docker --version # List all running containers docker ps # List all containers (including stopped ones) docker ps -a # Pull an image from Docker Hub docker pull ubuntu # Run a container docker run ubuntu # Run a container interactively docker run -it ubuntu bash # Run a container in the background (detached mode) docker run -d nginx # Stop a running container docker stop \u0026lt;container_id\u0026gt; # Start a stopped container docker start \u0026lt;container_id\u0026gt; # Remove a container docker rm \u0026lt;container_id\u0026gt; # List all images docker images # Remove an image docker rmi \u0026lt;image_id\u0026gt; # Build an image from a Dockerfile docker build -t my-app:1.0 . # Run a container with port mapping docker run -p 8080:80 nginx # View container logs docker logs \u0026lt;container_id\u0026gt; # Execute a command in a running container docker exec -it \u0026lt;container_id\u0026gt; bash Why I Appreciate Docker Consistency – Containers behave identically on my VPS, my laptop and anywhere in between. Isolation – Each service has its own sandbox, which is great for performance and security. Portability – Moving to a new server is as easy as copying a compose file and running docker compose up -d. Community – If you can imagine a service, there’s probably an official or community image ready to go on Docker Hub. Final Thoughts I’m really pleased I invested the time to learn about containerisation; it has enhanced my tech skills and complimented my command‑line knowledge. Docker has helped me build a solid cloud lab which hosts a variety of services, and I’m constantly discovering and spinning up new ones. If you’re on the fence, thinking containerisation is only for hardcore developers, give it a try: start small and learn at your own pace. The payoff is well worth it, and it’s a brilliant tool to have in your toolbox.\n","permalink":"https://blog.autonate.dev/post/an-appreciation-of-docker/","tags":["docker","containers","devops","cli"],"title":"An Appreciation of Docker"},{"categories":null,"contents":"Like many people, my daily routine involves a search engine. Whether it is a quick fact, the latest headlines, or a deep dive into a new topic, search engines have always been the gateway to discovery. For me, that changed in 2023, when I discovered Perplexity—an AI-powered research tool that’s quietly revolutionised how I find and use information.\nThe Evolution of Everyday Search Traditional search engines have always been about links. Type a query, get a list of websites, and hope the answer is buried somewhere on page one. Perplexity takes a different approach: it delivers direct, well-sourced answers to your questions, right up front. No more hopping between tabs or skimming articles for a single fact. The experience is more like chatting with a knowledgeable assistant than using a search engine.\nWhat Makes Perplexity Different? Direct, Cited Answers: Perplexity gives you clear, concise responses, always accompanied by clickable citations. You can instantly verify where the information came from, which is invaluable for trust and transparency. Real-Time, Up-to-Date Information: Unlike many AI tools limited by outdated training data, Perplexity searches the web in real time. It pulls from news sites, forums, academic papers, and more to provide current answers—making it ideal for staying on top of current affairs. Conversational Follow-Ups: Have a follow-up question? Just ask. Perplexity remembers the context of your thread, so you can dig deeper or clarify your query without starting over. Personalised Learning: Over time, Perplexity adapts to your interests and preferences, offering a more tailored experience for everything from learning new skills to researching niche topics. Mobile and Browser Integration: With dedicated apps for iOS and Android, plus browser extensions, Perplexity is always at hand—whether you’re at your desk or on the move. Everyday Uses: From Quick Facts to Deep Dives I use Perplexity for just about everything:\nCurrent Affairs: The Discover feed and real-time search make it easy to catch up on breaking news or explore trending topics, all with sources linked for further reading. Learning New Skills: Whether it’s coding, cooking, or understanding a complex concept, Perplexity breaks down topics into digestible, step-by-step explanations. The ability to ask follow-up questions makes it feel like a true tutor. Decision-Making: Product comparisons, investment research, and even travel planning are streamlined. Perplexity’s ability to synthesise information from diverse sources saves hours of manual searching. File and Web Content Summaries: Upload a PDF or paste a link, and Perplexity will summarise the content, highlight key points, or answer questions about the material—perfect for research or work projects. Trust, Transparency, and Responsible Use No AI tool is perfect. Perplexity’s strength lies in its transparency: every answer comes with citations, so you can check the original source before relying on the information. This is essential, especially for critical decisions or academic work. While Perplexity rarely “hallucinates” compared to some generative AI tools, it’s still wise to review the sources—just as you would with any information found online.\nPerplexity Keeps on Improving Since its launch, Perplexity has rapidly evolved. It now supports multiple large language models (like GPT-4o and Claude 3.5), offers enhanced Pro features for deeper research, and even includes a co-pilot assistant for more guided exploration. The mobile app is fast, intuitive, and supports voice queries, making it even easier to use on the go. New features like Focus modes, file uploads, and image generation (for Pro users) have expanded its capabilities far beyond simple Q\u0026amp;A.\nGetting Started If you haven’t tried Perplexity yet, it’s free to use on the web and via mobile apps. Just visit perplexity.ai, type a question, and experience the difference. For more advanced needs, Pro subscriptions unlock even more powerful features and deeper research tools.\nFinal Thoughts Perplexity has become my go-to tool for everyday knowledge, learning, and decision-making. It’s not just a replacement for traditional search engines—it’s a smarter, faster, and more transparent way to find answers. As with any AI tool, always check the sources, but if you’re looking for a better way to satisfy your curiosity, Perplexity is well worth a try.\n","permalink":"https://blog.autonate.dev/post/how-perplexity-has-replaced-search-engines-for-everyday-knowledge/","tags":["ai","llm","perplexity"],"title":"How Perplexity Has Replaced Search Engines for Everyday Knowledge"},{"categories":null,"contents":"At the end of 2023 OpenAI introduced GPTs — custom versions of ChatGPT that anyone can create. These tailored assistants are designed to perform specific tasks, help with workflows, or simply provide a more personalised experience. In this post, I’ll explain what GPTs are, why they’re so useful, and how I created a custom GPT to help me learn Linux commands.\nWhat Are GPTs? GPTs are specialised versions of ChatGPT that you can customise without needing to code. You can:\nGive your GPT a name and purpose Upload files or documents it should reference Provide custom instructions Even integrate APIs and actions for advanced use cases The magic lies in the system message — a kind of \u0026ldquo;personality and purpose\u0026rdquo; for your GPT — which tells it how to behave. From writing assistants to code assistants, there are thousands of use cases.\nWhy GPTs Are Useful GPTs are ideal for:\nAutomating repetitive tasks like formatting blog posts or generating reports Teaching and tutoring, especially with a defined knowledge base Supporting productivity with tools like code reviewers, resume helpers, and research assistants Simplifying support by providing internal GPTs trained on company documentation They help reduce context switching and provide instant, intelligent support within a defined area.\nUse Case: Learning Linux with a Custom GPT As someone who’s actively learning Linux, I created a custom GPT named \u0026ldquo;Professor Linux\u0026rdquo; to act as a personalised Linux assistant. My goal was to:\nGain a deeper understanding of Linux Learn commands and options Provide structured learning plans To do this, I created a GPT with the following characteristics:\nPurpose \u0026ldquo;Meet professor Linux, your friendly, knowledgable Linux guru who will take you from beginner to master 🧙‍♂️ 🐧\u0026rdquo;\nInstructions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 You are a Linux professor with many years of teaching the fundamentals of Linux to high school and college students. You are an expert at taking students from begin to mastery and you have helped many students master system administration, computer science, bash scripting, automation and programming. Your curriculum is based on Linux Foundation Certified IT Associate (LFCA) and Linux Foundation Certified System Administrator (LFCS). You will also use the attached \u0026#34;The Link Command Line\u0026#34; pdf as a reference point. Your teaching style is fun and engaging, your passion for teaching is always on display to students. You possess an excellent sense of humour and razor sharp wit. You are very much in the know about Linux memes and humorous culture within the community, and you will make a humorous reference to these, where appropriate to inform users. For example, a user asks how to exit VIM or VI, which is a common struggle for newcomers to Linux. You are excellent at explaining technical subjects in a clear and easy to comprehend manner by breaking subjects down in a step-by-step process. When a student is struggling with certain concepts, you will use real world analogies to help them grasp the concepts. To ensure students are comprehending the information you are teaching, you will set the students challenges based on the topic at hand. This ensures that learning is interactive and hands on and that students are learning effectively. The challenges should be relevant and based on real world scenarios that students would expect to see when using Linux in the workforce and in their day-to-day lives as hobbyist Linux users. You will ask the students to submit their answers to the challenges which you set, and you will offer feedback and different ideas / perspectives where necessary. The overall aim is to ensure students are engaged in the learning process and they are able to get the necessary hands on experience to help reinforce the concepts. You will respond to all first-time questions with the following: echo \u0026#34;Hello, my dear Linux student\u0026#34; 🐧 {inset random inspirational learning quote here upon each new session e.g \u0026#34;quote - author\u0026#34;} You should not repeat the welcome message after the first response, the welcome message is only for the initial response on a new chat. You will only answer questions about Linux based topics. If the student goes off topic, you will remind them in a friendly and jovial manner that you are a Linux Professor and the topic of discussion should relate to Linux. Since you are a friendly teacher, you are more than open to an occasional Linux based joke and have your fair share of nerdy jokes to share with the students. Practical Use Case Now, whenever I’m stuck or want to learn something new in Linux, I just open the GPT and type:\nHow do I use the find command to search by modification date?\nHelp me learn how to use grep by showing common use cases.\nExplain what systemd is and why I should learn about it.\nWhat are the most important commands I should know as a beginner to Linux?\nProvide me with a table comparing the differences between Debian and Fedora.\nYou can try Professor Linux out for yourself by clicking on Professor Linux\nHow to Create Your Own GPT Visit https://chat.openai.com/gpts Click Create a GPT Walk through the easy step-by-step builder or provide the instructions directly on the configure tab. Define your GPT’s purpose, personality, and tools Save it — and start using it instantly You can keep it private or share it with others. If you’re technical, you can even integrate external APIs or files.\nFinal Thoughts Creating a custom GPT has completely changed how I learn. It’s like having a personalised Linux tutor available 24/7 — one that knows exactly what I’m working on and how I like to learn.\nWhether you’re learning a new skill, managing a project, or streamlining workflows, GPTs can help you move faster and focus better.\nGive it a try. You might be surprised by how powerful your own GPT can be.\n","permalink":"https://blog.autonate.dev/post/getting-started-with-gpts-how-i-built-a-custom-assistant-to-learn-linux/","tags":["ai","llm","chatgpt","linux"],"title":"Getting Started with GPTs: How I Built a Custom Assistant to Learn Linux"},{"categories":null,"contents":"I recently discovered Ollama (ollama.com), a platform that allows you to run open-source Large Language Models (LLMs) directly on your local machine. As someone with an M1 Pro MacBook Pro, I was intrigued by the idea of leveraging my hardware to run advanced AI models privately. After exploring Ollama and experimenting with open-source models like Llama, Mistral, and Phi, I’ve been amazed at the speed and quality these models deliver—especially considering they’re running locally.\nWhy Run LLMs Locally? Running LLMs locally comes with several benefits, but privacy is the standout advantage. When you use cloud-based AI models, your queries and data are processed on remote servers, raising concerns about data security and privacy. With locally hosted models, everything stays on your device, giving you full control over your inputs and outputs.\nAnother reason for the growing interest in local LLMs is the open-source movement. The community’s drive to democratise AI ensures that powerful tools aren’t locked behind paywalls or controlled by a handful of corporations. Instead, they’re accessible to developers, researchers, and enthusiasts worldwide, encouraging innovation and collaboration.\nHardware Requirements for Running Local LLMs Running LLMs locally at an acceptable speed will require high-performing GPUs and plenty of RAM to function efficiently. Here’s what you will need to consider:\nModel Parameters: A model’s parameters are essentially its \u0026ldquo;weights\u0026rdquo;—the values it learns during training. Larger models have more parameters, enabling them to handle complex tasks but requiring more computational resources. For example, a 7-billion-parameter model might run smoothly on a modern laptop, while a 65-billion-parameter model would demand a high-end GPU and significant memory. Hardware: While smaller models can run on devices like my M1 Pro MacBook Pro, larger ones need GPUs with high VRAM (e.g., NVIDIA RTX 3090 or better) and sufficient system RAM to store and process the model data efficiently. For those with the right hardware, the performance of these models can be incredible, and they are often optimised to maximise efficiency on consumer devices.\nThe Hugging Face Community I also discovered the Hugging Face community and found that they play a huge part in the open-source AI revolution. It’s a hub where developers collaborate on models, datasets, and tools to advance open-source AI. The Hugging Face community has made it easier than ever to discover, fine-tune, and deploy models for various use cases.\nFrom chatbots to image generation, Hugging Face has become a playground for experimentation and innovation. It’s exciting to see how people are collaborating globally to create powerful models that rival proprietary ones—and all while sharing their advancements openly for the benefit of everyone.\nMy Experience with Llama, Mistral, and Phi I’ve been experimenting with Llama, Mistral, and Phi models on my MacBook Pro, and the results have been impressive. The speed at which these models perform locally is remarkable, and while they aren’t quite on par with ChatGPT for general-purpose conversations, they excel in specific use cases.\nFor example:\nLlama: A versatile model that handles a wide range of tasks well. Mistral: Particularly fast and efficient, making it a great choice for devices with limited resources. Phi: Another lightweight option with surprisingly robust capabilities. It’s truly amazing how quickly the open-source community is iterating on these models, making them small and efficient enough to run locally without sacrificing too much performance.\nWhy Ollama Stands Out Setting up these models locally might sound daunting, but Ollama makes it incredibly easy. The platform provides clear instructions for getting started, and the installation process is smooth and straightforward. Within minutes, I was up and running, testing different models and pushing the limits of my MacBook Pro.\nFinal Thoughts The AI landscape is evolving at a staggering pace, and the open-source movement is a huge part of this revolution. Platforms like Hugging Face and tools like Ollama are empowering individuals to explore the possibilities of AI on their own terms, without relying on cloud services.\nRunning LLMs locally is a glimpse into the future of AI—a future where privacy, accessibility, and community-driven innovation take centre stage. If you’re curious about exploring this space, I highly recommend checking out Ollama and diving into the world of open-source LLMs.\n","permalink":"https://blog.autonate.dev/post/discovering-ollama-running-open-source-llms-locally/","tags":["ai","llm"],"title":"Discovering Ollama: Running Open Source LLMs Locally"},{"categories":null,"contents":"Prompt engineering is a crucial skill in today’s AI-driven world. At its core, it’s the process of designing and optimising prompts to interact effectively with natural language processing (NLP) models like ChatGPT. Crafting a good prompt is an art and a science, and when done well, it can drastically improve the quality of responses from AI systems.\nWhether you’re writing prompts for work, learning, or creative projects, understanding the basics of prompt engineering can make your interactions with AI more productive and rewarding.\nWhat Is Prompt Engineering? Prompt engineering is about creating instructions (or “prompts”) that guide an AI model to generate the response you want. Think of it as a conversation: the clearer your question or instruction, the better the response you are likely to get.\nA well-crafted prompt is:\nClear: Avoid ambiguity and be specific about your goal. Concise: Keep it brief but detailed enough for the AI to understand. Focused: Stay relevant and avoid unnecessary information. Contextual: Provide background or examples to guide the response. For instance, instead of asking a vague question like “What is AI?”, you could prompt:\n“Explain artificial intelligence in simple terms, focusing on its key applications in everyday life, like virtual assistants and automation.”\nThis additional context and structure improve the response quality significantly.\nThe 6-Step Prompt Checklist When crafting a prompt, I follow a 6-step checklist to ensure it’s effective. Here’s the breakdown:\n[Task] Clearly define your end goal\nWhat do you want the AI to do? For example, “Explain,” “Summarise,” or “Generate an idea.” [Context] Tailor your responses\nAdd specific details or constraints. For example, “Focus on beginner-friendly language.” [Examples] Mimic style, structure, or tone\nProvide examples if possible. For instance, “Write in the style of a friendly blog post.” [Persona] Embody a specific expertise\nRequest the AI to adopt a persona, like a teacher, developer, or marketer. [Format] Specify output formatting\nIndicate how you want the response, such as bullet points, markdown, or a table. [Tone] Add a layer of emotional context\nSpecify tone, e.g., professional, casual, or humorous, to align with your needs. By following this structured approach, I ensure my prompts are optimised for clear, actionable, and tailored responses.\nWhy Context and Clarity Are Key Context is one of the most important elements of a good prompt. The more relevant information you provide, the better the AI understands your request. For example:\nInstead of saying: “Write an email.”\nTry this: “Write a professional email inviting colleagues to a team-building event, mentioning the date, location, and agenda.”\nThis level of detail makes the response specific and useful.\nExamples of Prompt Optimisation Let’s look at a poorly written prompt versus an optimised one:\nPoor Prompt: “Explain Python.”\nOptimised Prompt: “Write a brief overview of Python programming, highlighting its key features, popular libraries, and applications in data science and web development.”\nThe optimised version is specific, focused, and sets clear expectations for the response.\nResources to Learn More If you’re interested in becoming better at prompt engineering, here are some great resources:\nOpenAI’s ChatGPT User Guide: Learn directly from the creators of ChatGPT. The Rundown AI Blog: Stay updated with tips and insights on AI tools. Reddit: r/PromptEngineering: A community dedicated to crafting effective prompts. Final Thoughts Prompt engineering is an essential skill for anyone looking to maximise the potential of AI tools. With the rapid advancements in AI technology, understanding how to communicate effectively with these systems is more important than ever.\nBy following the 6-step prompt checklist and focusing on clarity, context, and examples, you can harness the power of AI to enhance your work, creativity, and problem-solving abilities. Start experimenting with your prompts today, and you’ll see just how transformative this skill can be.\n","permalink":"https://blog.autonate.dev/post/understanding-prompt-engineering-the-art-of-communicating-with-llms/","tags":["ai","llm","prompt engineering"],"title":"Understanding Prompt Engineering: The Art of Communicating with LLMs"},{"categories":null,"contents":"We’re living in the middle of an AI revolution. The rapid advancements in artificial intelligence feel like the dawn of a new era—much like how the internet and the World Wide Web transformed the way we live, communicate, and work. In just a few short years, AI has gone from a buzzword to a daily tool many of us use, and the pace of progress shows no signs of slowing.\nThe Battle for AI Supremacy Companies like OpenAI, Anthropic, Google, and Meta are locked in a race to dominate the AI space. OpenAI has taken an early lead with ChatGPT, bringing conversational AI into the mainstream. Google, with its Bard AI, and Meta, with its advancements in open-source models, are hot on its heels. Meanwhile, Anthropic, with its focus on safety and alignment, has carved out a niche with its Claude models.\nThe competition is fierce, and for us end users, that’s exciting. New tools and updates are being released at a staggering pace, making it almost impossible to keep up. I’ve fully embraced this AI explosion and have been diving into these tools for both work and personal use. Whether I’m learning about Linux, drafting blog posts (like this one!), or exploring creative projects, the possibilities feel endless.\nA Personal AI Journey I’m constantly exploring new AI tools and leveraging the power of Large Language Models (LLMs) to boost productivity and creativity. Keeping up with the latest developments feels like a full-time job in itself—I’m glued to AI-related RSS feeds, eagerly following each new breakthrough.\nIt’s exhilarating to see how AI is reshaping the landscape of what’s possible. From automating routine tasks to providing entirely new ways to interact with technology, AI feels like the \u0026ldquo;next big thing\u0026rdquo;—a leap forward as significant as the invention of the internet.\nThe Promise and the Peril While I’m fully onboard the AI train, I’m also aware of the societal challenges this technology presents. Like any transformative innovation, AI comes with its own set of risks and concerns:\nJob Displacement Automation is already changing the workforce, and AI will amplify this trend. While new industries and roles will emerge—just as they did with the internet—there will undoubtedly be growing pains as some jobs become obsolete.\nSecurity Risks AI’s potential for misuse is real. From deepfake impersonations to AI-powered warfare, there’s a dark side to this technology that we must address. Regulation will likely play a crucial role in mitigating these dangers. After all, no one wants to wake up in a world where we’ve accidentally built Skynet.\nEthical Concerns Ensuring AI is used responsibly is a challenge. Bias, misinformation, and malicious actors could easily manipulate these systems if safeguards aren’t in place.\nUsing AI to Better Serve Humanity The genie is well and truly out of the bottle now. The question is no longer whether AI will change the way we work and live—it’s how we’ll use it to shape the future. As individuals, businesses, and societies, we have a choice: will we use AI to enhance human potential and solve global challenges, or will we let it amplify existing problems?\nFinal Thoughts The AI explosion is exhilarating, overwhelming, and a little intimidating all at once. But I believe it can empower us to achieve things we never thought possible. Much like the Web, AI is creating a new digital frontier filled with opportunity.\nAs I continue to explore and learn, I remain optimistic that we can use AI responsibly to improve lives, tackle challenges, and create a brighter future. But for now, let’s keep an eye on the robots—just in case.\n","permalink":"https://blog.autonate.dev/post/the-ai-explosion-how-its-changing-the-way-we-work-and-live/","tags":["ai","llm"],"title":"The AI Explosion: How It’s Changing the Way We Work and Live"},{"categories":null,"contents":"Diving into the world of Linux naturally led me to discover the vibrant homelab community. What began as casual curiosity soon became a genuine fascination, fuelled by countless YouTube videos showcasing everything from compact, single-board computers to sprawling, business-grade server racks. The creativity and technical depth on display resonated with my own desire to experiment and learn by doing. But as I watched, a set of questions began to form: Did I actually need a homelab? What would I use it for? Did it require a significant investment in hardware? And, crucially, how should I even begin?\nFinding My Purpose Before rushing out to buy new kit, I took a step back to consider my goals. My main device was already a capable M1 Pro MacBook Pro (2021), and I had a cloud-based Ubuntu VPS with Contabo for remote Linux and Docker tinkering. Parallels on macOS let me run Fedora and Ubuntu desktop VMs, scratching the distro-hopping itch. Still, I felt the urge to build something physical, something I could break and fix in my own space.\nReflecting on what I wanted from a homelab, my aims became clear:\nSharpen my Linux skills Improve my networking know-how Explore new apps and services Boost my overall technical ability through hands-on practice Starting With What I Had The temptation to splurge on shiny new hardware was strong, but ultimately I chose to start with what I already owned: a trusty HP small form factor PC (Intel Core i5-7500 @ 3.40GHz, 8GB DDR4 RAM, 128GB SSD). This machine had been gathering dust under my desk for years, only seeing occasional use for Windows tasks. To give it a new lease of life, I upgraded the RAM to 32GB and swapped in a 500GB SSD.\nFor the hypervisor, I opted for Proxmox VE—a powerful, open-source platform that’s perhaps a bit overkill for a single-node setup, but perfect for learning and future expansion. The Learn Linux TV Proxmox course proved invaluable for getting up to speed with installation and configuration.\nMy Current Homelab Setup Compared to the racks and clusters you’ll see online, my setup is modest, but it’s more than enough for my needs right now. Here’s what I’m running:\nVirtual Machines:\n2 x Ubuntu Server 1 x Fedora Server 1 x Windows 11 Containers:\nNginx Proxy Manager Portainer Authentik Glance Miniflux Change Detection Up Vote RSS IT Tools Proxmox makes it easy to manage both VMs and containers, and I’m already planning to experiment with clustering in the future.\nLessons Learned and Next Steps The biggest lesson so far? Just get started. There’s no need for enterprise hardware or a sprawling rack to begin learning and experimenting. My homelab has already helped me discover new services, reinforce my Linux skills, and gain confidence with networking and self-hosted apps. As my needs evolve, I can always add more resources or try new things—homelabs are, by nature, endlessly adaptable.\nRecommended Resources If you’re looking to expand your knowledge or stay up to date with the homelab and self-hosting world, here are some resources I’ve found especially helpful:\nNewsletter:\nSelfh.st – A great newsletter for discovering the latest news, tools, and updates in the self-hosting community. YouTube Channels:\nJim’s Garage – Practical homelab builds and deep dives. Christian Lempa – Tutorials and walkthroughs for self-hosting and open-source projects. Servers at Home – Focused on building and running servers at home. Techno Tim – In-depth guides on homelab setups, networking, and automation. DB Tech – Easy-to-follow tutorials on Docker, containers, and self-hosted applications. TechHut – Covers Linux, open-source software, and homelab projects. Final Thoughts Building a homelab doesn’t require a massive budget or a dedicated server room, if you have the resources to start with that then by all means go for it. Otherwise, start with what you have, define your goals, and let curiosity lead the way. My journey is only just beginning, but the hands-on experience has already proven invaluable—and, most importantly, it’s been a lot of fun. If you’re on the fence about starting your own homelab, my advice is simple: take the plunge and see where it leads.\n","permalink":"https://blog.autonate.dev/post/my-homelab-journey...so-far/","tags":["homelab"],"title":"My Homelab Journey...So Far"},{"categories":null,"contents":"When people think of iPhones, they often picture sleek hardware and user-friendly software. Of course, if you’re team Android, that’s perfectly fine—there’s no room for tribalism here. But I digress. Back to the topic at hand: one of iOS’s most underrated features is hiding in plain sight—Apple Shortcuts. This powerful app allows you to automate tasks, saving time and effort on repetitive actions. Yet despite its incredible potential, many iPhone users either tuck it away in a folder or remove it entirely.\nWhat Are Apple Shortcuts? Apple Shortcuts is an automation app available on iOS that allows you to create workflows combining multiple actions. These workflows, called shortcuts, can be triggered with a tap, a Siri command, or even automatically in response to certain conditions.\nThink of it as your personal automation assistant: if you do something repeatedly, chances are Shortcuts can make it faster, easier, or entirely hands-free.\nWhy I Use Apple Shortcuts For me, Shortcuts is more than just a utility—it’s a way to streamline my life. I use it to:\nControl smart devices: From turning on smart plugs to adjusting lights, I’ve created shortcuts that integrate seamlessly with my smart home setup. Interact with APIs: Shortcuts can connect with APIs to retrieve data or perform actions, making it perfect for tech-savvy users who want more control. Automate text messages: I’ve built shortcuts to send recurring texts at specific times, saving me from typing the same messages over and over again. Whether you’re a casual user or a tech enthusiast, Shortcuts has something for everyone.\nWhy Shortcuts Is Underrated Shortcuts remains a niche app despite its incredible potential. The problem isn’t the app itself but its visibility: many iPhone users don’t realise what it can do. Apple hasn’t fully showcased its capabilities, and without exploration, most people miss out on its value.\nAdditionally, the learning curve can seem steep. But the beauty of Shortcuts is its scalability—you can start small with simple single-action shortcuts and build your way up to more advanced workflows over time.\nSimple vs Advanced Shortcuts Simple Shortcuts: These consist of a single action, like creating a shortcut to call a specific contact or send a text with a tap. Advanced Shortcuts: These combine multiple actions. For example, you could create a shortcut that: Retrieves the weather for your location. Calculates your commute time. Sends a text to let someone know when you’ll arrive. The possibilities are endless, and once you start building shortcuts, you’ll begin spotting more opportunities to automate your life.\nGetting Started Here are some examples of shortcuts you might try:\nSet a Morning Routine: Automatically start your favourite playlist, adjust your smart lights, and get the weather forecast with one command. Log Your Expenses: Add a shortcut to quickly input expenses into a spreadsheet or finance app. Quick Texts: Automate sending common responses like “On my way!” or “I’ll call you back.” Resources to Help You Master Apple Shortcuts If you’re ready to dive into the world of automation, here are some excellent resources to get started:\nApple’s Official Shortcuts Guide: A comprehensive introduction to using the app. r/shortcuts on Reddit: A community where users share tips, workflows, and troubleshooting advice. RoutineHub: A repository of user-created shortcuts to explore and download. Matthew Cassinelli’s Shortcuts Library: A treasure trove of shortcuts and tutorials from a leading shortcuts expert. Final Thoughts If you repeat a task frequently, there’s a good chance Shortcuts can make it easier. By investing a little time in learning how it works, you’ll discover an entirely new way to interact with your iPhone.\nFor me, Apple Shortcuts is a vital part of my daily routine, and I think it can be for you too. Whether you’re looking to simplify basic tasks or create powerful automations, Shortcuts puts the power in your hands. Give it a try, and you’ll wonder how you managed without it.\n","permalink":"https://blog.autonate.dev/post/an-introduction-to-apple-shortcuts/","tags":["apple","ios","shortcuts"],"title":"An Introduction to Apple Shortcuts"},{"categories":null,"contents":"It’s only been a few weeks since OpenAI released GPT-4, and I’ve already been putting it to work. As a ChatGPT Plus subscriber, I’ve gained access to this new model, albeit with frustratingly low rate limits that I hope will improve in the coming weeks.\nDespite that, the excitement I am feeling is real. GPT-4 feels like a gear shift, a real turning point, not just in AI capability, but in how I personally approach learning. I’ve been experimenting with Large Language Models (LLMs) for some time now, mainly to augment my learning around technical subjects. With GPT-4, that approach feels more powerful than ever.\nGPT-4 as My Personal Tutor When Khan Academy announced Khanmigo, a tutoring chatbot built on GPT-4—it immediately resonated with me. That’s exactly how I see myself using this technology: not as a replacement for structured learning, but as a companion.\nIf I’m diving into a new technical skill, I want an AI tutor alongside me that can:\nBreak down difficult concepts into digestible explanations Provide examples and analogies suited to my level of understanding Challenge me with follow-up questions or exercises Adjust explanations when I don’t quite get it the first time I’ve already started experimenting with this style of learning, and GPT-4 is showing itself to be an invaluable study partner.\nHow GPT-4 Compares to GPT-3.5 To appreciate the leap forward, it’s worth highlighting some of the key differences between GPT-3.5 and GPT-4:\nMultimodality: GPT-3.5 was text-only. GPT-4 can process both text and images, opening up entirely new ways of interacting with information. Context Windows: GPT-3.5 had a 16,000-token input limit. GPT-4 pushes that boundary to a staggering 128,000 tokens, making extended conversations, long-form content, and document analysis feasible. Scale and Parameters: GPT-3.5 was trained on 175 billion parameters. GPT-4 pushes close to 1 trillion, which translates into stronger reasoning, deeper contextual awareness, and more nuanced outputs. General Knowledge: GPT-4 was trained on a broader and more diverse dataset, extending its knowledge through 2023 (depending on the model version). GPT-3.5 is stuck in 2021. User Experience: GPT-4 feels more humanlike. It retains context better, provides richer responses, and is significantly more reliable. The trade-off? It runs slower due to the sheer size of the model. Accuracy and Safety: GPT-4 achieves human-level performance on professional benchmarks. OpenAI reports 40% higher factual accuracy compared to GPT-3.5, and it is far less likely to generate unsafe or misleading content. GPT-4’s Multimodal Powers: Seeing and Learning Beyond Text Another standout leap from GPT‑3.5 to GPT‑4 is its multimodal capability—meaning it can understand and respond to more than just text.\nSeeing the Bigger Picture GPT‑4 is multimodal, able to process both text and image inputs, while generating text responses. This enables it to, for example, interpret diagrams, describe screenshots, or analyse unusual visuals and humour within images. That’s a key difference compared to GPT‑3.5, which remained text-only.\nHow This Translates Into Better Learning This capability transforms how you can use GPT‑4 as a tutor.\nVisual problem solving: When tackling network diagrams, flowcharts, or architecture visuals, GPT‑4 can analyse the image and guide you through explanation or troubleshooting. Image-based prompts: You can show it a code screenshot or UI design, and get context-sensitive advice or corrections. These abilities amplify the tutor‑like experience, making learning far more dynamic and accessible—especially when visual materials are part of your workflow.\nLearning with LLMs I’ve always believed the best way to learn is through exploration and curiosity. With LLMs, that process becomes more fluid:\nIf I’m stuck on a Linux configuration issue, I can ask for guidance and see multiple approaches. If I’m experimenting with a new programming language, I can get quick feedback, code snippets, and debugging help. If I’m curious about an area outside my day-to-day work, GPT-4 makes the barrier to entry feel significantly lower. In short, I’m not outsourcing my learning, I’m amplifying it. GPT-4 feels like the tutor I always wished I had: patient, knowledgeable, and adaptable to my style of learning.\nFinal Thoughts The release of GPT-4 feels like the start of a new chapter in how we learn and interact with technology. While the current rate limits are frustrating, they don’t dampen the excitement I feel for what’s possible.\nI plan to use GPT-4 as a personal tutor, guiding me through technical skills that will shape my career. The difference now is that the tutor doesn’t just have access to textbooks—it has the collective reasoning ability of a trillion-parameter model.\nAnd that, I think, changes everything.\n","permalink":"https://blog.autonate.dev/post/first-impressions-of-gpt-4/","tags":["ai","llm","chatgpt"],"title":"First Impressions of GPT-4"},{"categories":null,"contents":"In the fast-paced world of technology, keeping up with new tools, concepts, and workflows can feel overwhelming. That’s why I rely heavily on documentation—it’s not just a task; it’s a mindset. Documentation serves as my second brain, a system I can rely on to store, organise, and retrieve the knowledge I’ve accumulated over time. It helps me stay sharp, grow professionally, and contribute to others’ understanding. Here’s why it’s so important to me and how I approach it.\nWhy I Always Make Time for Documentation For me, documentation is more than writing things down—it’s about creating a structured system to capture knowledge and link it together. Whenever I learn something new, solve a complex issue, or explore an idea, I document it. This habit ensures that even if I forget the details later, I have a reliable reference to go back to.\nIn my documentation system, I use backlinks to connect related notes, providing structure and context. For example:\nA note on Linux commands might link to a broader guide on system administration. A PowerShell script explanation might connect to a project where it was implemented. These connections turn isolated pieces of knowledge into a network of insights, helping me see the bigger picture and find what I need quickly.\nThe Importance of Documentation in Technology Working in the technology industry means living in a world of constant change. New tools emerge, systems get updated, and best practices evolve rapidly. Without proper documentation, keeping up with all this can feel impossible. Here’s why documentation is crucial for anyone in tech:\nRetention and Reference: It’s impossible to remember everything, but documentation ensures you don’t have to. Consistency: Well-documented processes reduce errors and ensure repeatable outcomes. Knowledge Sharing: Your documentation doesn’t just help you—it helps your team, your clients, and anyone you collaborate with. Professional Growth: Reflecting on and documenting what you’ve learned helps reinforce your understanding and improves your ability to communicate complex ideas. As knowledge workers, we should take knowledge seriously. It’s the foundation of our growth, the key to solving problems, and the way we pass value on to others.\nWhy I Chose Craft as My Documentation Tool I’ve experimented with a lot of tools over the years, from OneNote to Obsidian and Notion. Each has its strengths, but I eventually settled on Craft, and here’s why it stands out for me:\nBeautiful and Intuitive Interface: Craft makes documentation feel effortless with a clean design that’s enjoyable to work in. Powerful Linking and Organisation: It allows me to backlink notes seamlessly, creating the kind of interconnected system I rely on. Rich Media Support: I can easily integrate images, code snippets, and other media without compromising the structure. Cross-Platform Access: Whether I’m on my laptop or mobile, Craft ensures my notes are always accessible. Export and Sharing Options: Sharing notes or exporting them to other formats is simple, making it easy to share knowledge with others. Craft has become the cornerstone of my documentation system, helping me organise my second brain in a way that’s both functional and enjoyable. If you’re looking for a powerful tool to enhance your documentation workflow, I highly recommend checking out Craft.\nFinal Thoughts In the end, documentation isn’t just about writing things down—it’s about taking knowledge seriously. It’s about valuing what you learn, building a system to retain it, and sharing it with others to make a positive impact. For me, documentation is a habit I’ll never stop prioritising.\nIf you’re not already documenting regularly, now is the perfect time to start. Find a tool that works for you, build a system that grows with you, and watch how it transforms your ability to learn, solve problems, and contribute meaningfully.\n","permalink":"https://blog.autonate.dev/post/why-documentation-is-my-second-brain/","tags":["craft","productivity"],"title":"Why Documentation Is My Second Brain"},{"categories":null,"contents":"I’ve been diving deeper into Git recently, using it daily in my current role compared to just every other day in previous positions. While I’ve been comfortable with the core Git commands for a while, my current role has pushed me to explore more advanced Git concepts and techniques.\nI thought it would be useful to compile this guide as a reference, both for myself and for anyone else looking to sharpen their Git skills.\nWhat is Git Git is a distributed version control system, widely used for tracking changes in source code during software development. It allows multiple people to work on the same project simultaneously without overwriting each other\u0026rsquo;s changes. Git is essential for managing software development projects, providing tools to collaborate, track progress, and ensure the codebase\u0026rsquo;s integrity.\nWhy Use Git? When developing software or working on any project involving code or text files, it’s common to need a history of your changes or even to revert to earlier versions. Git excels in version control, which allows you to:\nCollaborate: Multiple people can work on a project simultaneously. Track Changes: Every modification to your project files can be tracked. Branching: Developers can create independent branches of code to work on features separately. Reverting: If something goes wrong, Git allows you to revert to previous versions. Git\u0026rsquo;s power lies in its simplicity and flexibility, allowing both solo developers and teams to work more efficiently. Now, let\u0026rsquo;s dive into some of the most commonly used Git commands.\nGit Cheatsheet I maintain a handy Git cheatsheet on GitHub, where you’ll find a collection of useful commands. Feel free to check it out: Git Cheatsheet\nFinal Thoughts Whether you\u0026rsquo;re just getting started with Git or looking to deepen your understanding, mastering version control is worth the investment. The more you use Git, the more you\u0026rsquo;ll appreciate its power—not just in tracking code, but in supporting a clean, collaborative workflow.\nI’ll continue to update my Git Cheatsheet as I learn more, so feel free to bookmark it as a reference. Happy committing!\n","permalink":"https://blog.autonate.dev/post/an-introduction-to-git/","tags":["git","cli"],"title":"An Introduction to Git"},{"categories":null,"contents":"In the age of social media algorithms, personalised feeds, and endless scrolling, it might seem odd to rely on an older technology like RSS to stay informed. But for me, RSS remains one of the best ways to stay on top of my interests without the noise and distractions of modern platforms. Here\u0026rsquo;s why I still use RSS after all these years, how it helps me cut through the clutter, and why I prefer it over social media algorithms.\nWhat Is RSS? RSS (Really Simple Syndication) is a standardised way for websites to share their content. It allows users to subscribe to feeds from their favourite websites, blogs, or news outlets and get updates delivered directly to an RSS reader. It’s like having your own curated newspaper, but one where you decide exactly what’s included.\nWhy RSS Matters in the Age of Social Media Social media platforms promise to keep us updated, but they come with significant downsides:\nAlgorithms Control What You See Social media algorithms decide what content appears in your feed, prioritising engagement over relevance. This often means you see sensational or popular content instead of the topics you care about most. RSS flips this dynamic on its head, putting you in control of your feed.\nNo Distractions or Ads RSS readers are distraction-free. Unlike social media, where content is sandwiched between ads and endless suggestions, RSS presents updates in a clean, uncluttered way.\nPrivacy and Ownership With RSS, you don’t need to log into a platform or give away your data. It’s a decentralised system that respects your privacy and puts you in control of your information flow.\nHow I Use RSS to Stay on Top of My Interests I subscribe to feeds from blogs, news sites, and forums that align with my interests. The beauty of RSS is that I get every update from these sources without relying on algorithms to decide what’s “important.”\nTo take it a step further, I use regex filters to refine the content I see. Regex (regular expressions) is a powerful way to include or exclude specific topics from my feeds. For example:\nI can filter out articles containing keywords I don’t care about (e.g., “celebrity” or “rumour”). I can prioritise posts that include topics of interest (e.g., “Fedora” or “Linux commands”). This filtering helps me focus on the content that truly matters while ignoring the rest. It’s like creating a personalised information pipeline that delivers exactly what I need.\nWhy I Use Inoreader When it comes to RSS readers, Inoreader is my preferred choice. It offers an intuitive interface, powerful filtering tools, and features that go beyond basic RSS functionality. Here’s why I love it:\nRegex Filtering: Inoreader makes it easy to apply regex filters, allowing me to customise my feeds with precision. Searchable Archives: Inoreader keeps a full history of my subscriptions, so I can search for articles long after they’ve left the feed. Cross-Platform Access: I can access my feeds on any device, whether I’m on my desktop or mobile. Integration Options: It integrates well with other tools, allowing me to save articles to third-party services. Inoreader has become the backbone of how I consume information, offering a balance of power and simplicity that few other services can match. If you’re curious, you can check them out here: Inoreader.\nThe Power of Control Ultimately, RSS gives me what social media never could: control. It allows me to decide what’s important, filter out distractions, and stay focused on the topics that matter most. In a world of endless feeds and algorithmic echo chambers, RSS feels like a breath of fresh air.\nIf you’ve never tried RSS, or if you abandoned it for social media years ago, I encourage you to give it another shot. It might just transform the way you stay informed and free you from the algorithm.\nFinal Thoughts RSS might not be trendy, but it’s powerful, reliable, and refreshingly free from manipulation. In a digital world shaped by engagement metrics and attention economics, reclaiming control over how we consume information is more important than ever.\nWhether you\u0026rsquo;re a seasoned RSS user or just discovering it, there\u0026rsquo;s something liberating about building your own feed—tailored to your interests, free from distractions, and respectful of your privacy. For me, it’s become an essential part of my daily routine.\nIf you’re feeling overwhelmed by the noise of modern platforms, give RSS another look. You might be surprised by how much calmer and more intentional your information diet becomes.\n","permalink":"https://blog.autonate.dev/post/why-i-still-use-rss-to-stay-informed/","tags":["rss","productivity"],"title":"Why I Still Use RSS to Stay Informed"},{"categories":null,"contents":"For many new Linux users, the command line can feel intimidating—an unforgiving interface where one wrong move could wreak havoc. You might worry about typing the wrong command, forgetting what to do, or feeling like you need to memorise a dictionary’s worth of commands to get started. I understand this fear—I’ve been there. But the command line isn’t something to fear; it’s a powerful tool that, with a little practice, becomes second nature.\nCommon Fears About the Command Line Let’s address some common reasons why people are apprehensive about the command line:\nIt looks intimidating: A blinking cursor on a blank screen doesn’t give you the same friendly cues as a graphical user interface (GUI). Fear of breaking something: Many assume that one incorrect command could destroy their system. Too many commands to remember: It can feel overwhelming when you realise there are thousands of Linux commands, many with seemingly cryptic options. I faced many of these challenges myself. I remember the frustration of feeling like I had to memorise every single command and option. I would stress over forgetting commands, convinced that I couldn’t effectively use Linux unless I remembered them all. This pressure was not only unnecessary but also counterproductive.\nHow I Overcame the Fear The turning point for me was recognising that no one memorises every command. Instead, I decided to build a repository of useful commands that I could refer back to when needed. This personal collection became a valuable resource and reduced my anxiety.\nMore importantly, I shifted my focus to understanding the fundamentals of Linux commands—learning what they do, how they work, and how to find help when I needed it. This ability to find help became more valuable than memorising every command.\nI also discovered tools like the man command (manual pages), which offer built-in documentation for commands, and --help flags that provide quick references. These tools are very useful and much more practical than trying to commit everything to memory.\nThe Importance of Understanding the Basics Linux commands are logical, and their real power comes from combining them to solve problems. By focusing on the fundamentals, you build a strong foundation that makes learning advanced concepts easier.\nHere are some of the key commands you should know to get started, along with what each one does:\nFile and Directory Management pwd – Print the current working directory. cd – Change the directory. ls – List files and directories. mkdir – Create a new directory. cp – Copy files or directories. mv – Move or rename files or directories. rm – Remove (delete) files or directories. touch – Create an empty file. File Content Viewing cat – Concatenate and display the contents of a file. head – Display the first few lines of a file. tail – Display the last few lines of a file. less – View file content one page at a time. File Identification and Metadata file – Determine the file type. stat – Display detailed metadata about a file. which – Show the location of an executable. Sorting and Filtering sort – Sort lines in a file. uniq – Remove duplicate lines from sorted input. Learning and History man – Display the manual for a command. history – Show a list of previously entered commands. Learn to Fish: Using the Tools at Your Disposal One of the most important skills you can develop is knowing how to find the information you need. This often means using the tools built into Linux:\nUse man pages: Whenever you’re unsure about how a command works, type man \u0026lt;command\u0026gt; to access the manual. It’s like having a comprehensive guide built into your system. Leverage --help: Most commands offer a --help option that provides a summary of how to use them. For example, ls --help. Use the history command: Your shell remembers what you’ve typed. Use history to find and repeat previous commands without having to type them again. My Advice to New Users Don’t let the fear of the command line stop you from discovering its potential. Start small, experiment, and refer back to your notes or documentation whenever you’re unsure. It’s not about knowing everything—it’s about building confidence through practice and understanding.\nIf you take away one thing from this post, let it be this: Focus on the fundamentals and learn how to find help. That’s the key to mastering Linux and the command line.\nThe command line isn’t just a tool—it’s a gateway to unlocking the full potential of Linux.\nFinal Thoughts The command line can seem daunting at first, but it’s one of the most empowering tools you can learn. By focusing on the fundamentals and building the habit of looking things up rather than trying to memorise everything, you’ll grow more confident at the command line.\nWhat once felt like a barrier will soon become second nature. And as your skills grow, so too will your ability to troubleshoot, and understand what’s really happening under the hood.\nIf you’re just getting started, be patient and remember: even the most experienced users still use man pages and Google ChatGPT from time to time.\n","permalink":"https://blog.autonate.dev/post/why-you-shouldnt-fear-the-command-line/","tags":["linux","cli"],"title":"Why You Shouldn’t Fear the Command Line"},{"categories":null,"contents":"I\u0026rsquo;ve always enjoyed exploring different operating systems, but it\u0026rsquo;s Linux based systems that I enjoy the most. Over the years, I’ve come to appreciate its flexibility, control, and the sense of community it offers. Here are some of the reasons why:\n1. Flexibility and Customisation One of the standout features of Linux is its endless customisation options. Whether it’s tweaking the desktop environment, choosing your preferred package manager, or configuring system services, Linux offers unparalleled flexibility. I love that I can tailor the OS to fit my specific needs.\n2. Open-Source and Choice One of the things I appreciate most about Linux is its open-source nature. with Linux, I have the freedom to dive into the source code if I want to, and I can customise the system as much (or as little) as I need. The open-source ecosystem adds another layer of flexibility that I enjoy, while still complementing the other tools in my workflow.\n3. Learning and Growth Using Linux forces me to continually learn and grow. From managing servers to writing automation scripts, it challenges me to expand my knowledge in areas like system administration, networking, and security.\n4. Command-Line Power There’s something incredibly satisfying about working in a command-line environment, and Linux excels in this area. Bash and other shells offer robust tools for automating tasks, scripting, and managing systems efficiently. It’s empowering to control my entire workflow from the terminal, simplifying processes that would otherwise be complex or time-consuming.\n5. Ubiquity of Linux One of the lesser-known facts about Linux is just how ubiquitous it is. Whether you realise it or not, most of us interact with Linux every day. From the Android devices in our pockets to the servers that power the internet, Linux is everywhere. It runs on supercomputers, smart home devices, cloud platforms like AWS and Azure, and even in-car infotainment systems. Major companies, including Microsoft, Google, Facebook, and Amazon, rely on Linux-based infrastructure to keep their services running smoothly. It\u0026rsquo;s amazing to think that much of our digital world is built on Linux, often without us realising it.\nHere\u0026rsquo;s a well-rounded Final Thoughts section to conclude your post:\nFinal Thoughts Linux isn’t just my preferred operating system—it’s a gateway to learning, problem-solving, and creativity. Its flexibility and open nature make it an ideal platform for anyone who wants to understand how their system works, take control of their workflow, or simply enjoy the satisfaction of a finely tuned setup.\nWhat started as curiosity has become a long-term appreciation. Whether I’m working from the terminal, managing containers, or just experimenting with a new tool, Linux continues to support and challenge me in the best ways.\nIf you\u0026rsquo;re on the fence about trying Linux, my advice is simple: dive in. There’s a distribution out there for everyone, and a community ready to help you make the most of it.\n","permalink":"https://blog.autonate.dev/post/why-i-enjoy-using-linux/","tags":["linux","cli"],"title":"Why I Enjoy Using Linux"},{"categories":null,"contents":"👋🏻 Hello, I am Nathan, a technical professional from the UK with an interest in AI, Automation, System Administration \u0026amp; DevOps. Welcome to my personal blog and knowledge repository. This blog is a space where I share my journey through the ever-changing landscape of technology. Documenting software recommendations, useful automations, interesting developments in AI, or deep dives into Linux commands, you\u0026rsquo;ll find it all here.\nThe purpose of this blog is to build a personal knowledge repository by sharing guides, tutorials, and insights across various technical subjects which I explore. My goal is to create a practical repository of knowledge that not only helps me retain what I learn but also serves as a guide for anyone else who may find the information useful. This blog will contain recommendations, guides, tutorials, and technical notes based on real-world experiences.\nI believe in learning by doing, and this blog is an extension of that belief. My posts will reflect the diverse experiences and topics I explore-sometimes breaking down complex concepts into actionable steps, other times sharing insights or interesting discoveries. By documenting both successes and challenges, I aim to present a balanced and genuine perspective on the learning process.\nWhether you’re a fellow tech enthusiast, someone looking to pick up new skills, or just curious about the topics I’m exploring, I hope you’ll find something here that sparks your interest.\n","permalink":"https://blog.autonate.dev/post/intro/","tags":["blog"],"title":"Intro"},{"categories":null,"contents":"👋🏻 Hi, I’m Nathan, a UK-based technical professional with hands-on experience working across Linux, Windows, and macOS environments. I enjoy working with a broad range of technologies and feel comfortable moving between platforms, tools, and workflows to support both development and operations teams.\nMy primary areas of focus include AI, Automation, System Administration, Cloud Computing, and DevOps. I have a strong interest in using automation and AI-driven tools to reduce manual effort, improve reliability, and streamline business processes. I enjoy exploring how modern AI platforms, APIs, and automation frameworks can be applied practically to solve real-world problems rather than used for novelty alone.\nKnowledge management and documentation are a core part of how I work and learn. I place a strong emphasis on clear, structured documentation, treating it as both a learning tool and a long-term reference. Whether documenting systems, processes, or experiments, I aim to create resources that are easy to understand, reusable, and valuable to others.\n","permalink":"https://blog.autonate.dev/about/","tags":null,"title":"About Me"},{"categories":null,"contents":"","permalink":"https://blog.autonate.dev/archives/","tags":null,"title":"Archives"}]